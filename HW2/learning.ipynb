{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Table of Contents\n",
    "1. [Imports](#Imports)\n",
    "2. [Data Read In](#Data-Read-in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports\n",
    "[back to top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import unittest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED\n",
    "            )\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.has_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Part 1\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"Input: takes in a list of words, and a dictionary containing the index of the words\n",
    "    Output: a tensor containing the indexes of the word\"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "# This is the example training data\n",
    "training_data = [\n",
    "    (\"the dog happily ate the big apple\".split(), [\"DET\", \"NN\", \"ADV\", \"V\", \"DET\", \"ADJ\", \"NN\"]),\n",
    "    (\"everybody read that good book quietly in the hall\".split(), [\"NN\", \"V\", \"DET\", \"ADJ\", \"NN\", \"ADV\", \"PRP\", \"DET\", \"NN\"]),\n",
    "    (\"the old head master sternly scolded the naughty children for \\\n",
    "     being very loud\".split(), [\"DET\", \"ADJ\", \"ADJ\", \"NN\", \"ADV\", \"V\", \"DET\", \"ADJ\",  \"NN\", \"PRP\", \"V\", \"ADJ\", \"NN\"]),\n",
    "    (\"i love you loads\".split(), [\"PRN\", \"V\", \"PRN\", \"ADV\"])\n",
    "]\n",
    "#  These are other words which we would like to predict (within sentences) using the model\n",
    "other_words = [\"area\", \"book\", \"business\", \"case\", \"child\", \"company\", \"country\",\n",
    "               \"day\", \"eye\", \"fact\", \"family\", \"government\", \"group\", \"hand\", \"home\",\n",
    "               \"job\", \"life\", \"lot\", \"man\", \"money\", \"month\", \"mother\", \"food\", \"night\",\n",
    "               \"number\", \"part\", \"people\", \"place\", \"point\", \"problem\", \"program\",\n",
    "               \"question\", \"right\", \"room\", \"school\", \"state\", \"story\", \"student\",\n",
    "               \"study\", \"system\", \"thing\", \"time\", \"water\", \"way\", \"week\", \"woman\",\n",
    "               \"word\", \"work\", \"world\", \"year\", \"ask\", \"be\", \"become\", \"begin\", \"can\",\n",
    "               \"come\", \"do\", \"find\", \"get\", \"go\", \"have\", \"hear\", \"keep\", \"know\", \"let\",\n",
    "               \"like\", \"look\", \"make\", \"may\", \"mean\", \"might\", \"move\", \"play\", \"put\",\n",
    "               \"run\", \"say\", \"see\", \"seem\", \"should\", \"start\", \"think\", \"try\", \"turn\",\n",
    "               \"use\", \"want\", \"will\", \"work\", \"would\", \"asked\", \"was\", \"became\", \"began\",\n",
    "               \"can\", \"come\", \"do\", \"did\", \"found\", \"got\", \"went\", \"had\", \"heard\", \"kept\",\n",
    "               \"knew\", \"let\", \"liked\", \"looked\", \"made\", \"might\", \"meant\", \"might\", \"moved\",\n",
    "               \"played\", \"put\", \"ran\", \"said\", \"saw\", \"seemed\", \"should\", \"started\",\n",
    "               \"thought\", \"tried\", \"turned\", \"used\", \"wanted\" \"worked\", \"would\", \"able\",\n",
    "               \"bad\", \"best\", \"better\", \"big\", \"black\", \"certain\", \"clear\", \"different\",\n",
    "               \"early\", \"easy\", \"economic\", \"federal\", \"free\", \"full\", \"good\", \"great\",\n",
    "               \"hard\", \"high\", \"human\", \"important\", \"international\", \"large\", \"late\",\n",
    "               \"little\", \"local\", \"long\", \"low\", \"major\", \"military\", \"national\", \"new\",\n",
    "               \"old\", \"only\", \"other\", \"political\", \"possible\", \"public\", \"real\", \"recent\",\n",
    "               \"right\", \"small\", \"social\", \"special\", \"strong\", \"sure\", \"true\", \"white\",\n",
    "               \"whole\", \"young\", \"he\", \"she\", \"it\", \"they\", \"i\", \"my\", \"mine\", \"your\", \"his\",\n",
    "               \"her\", \"father\", \"mother\", \"dog\", \"cat\", \"cow\", \"tiger\", \"a\", \"about\", \"all\",\n",
    "               \"also\", \"and\", \"as\", \"at\", \"be\", \"because\", \"but\", \"by\", \"can\", \"come\", \"could\",\n",
    "               \"day\", \"do\", \"even\", \"find\", \"first\", \"for\", \"from\", \"get\", \"give\", \"go\",\n",
    "               \"have\", \"he\", \"her\", \"here\", \"him\", \"his\", \"how\", \"I\", \"if\", \"in\", \"into\",\n",
    "               \"it\", \"its\", \"just\", \"know\", \"like\", \"look\", \"make\", \"man\", \"many\", \"me\",\n",
    "               \"more\", \"my\", \"new\", \"no\", \"not\", \"now\", \"of\", \"on\", \"one\", \"only\", \"or\",\n",
    "               \"other\", \"our\", \"out\", \"people\", \"say\", \"see\", \"she\", \"so\", \"some\", \"take\",\n",
    "               \"tell\", \"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \"these\",\n",
    "               \"they\", \"thing\", \"think\", \"this\", \"those\", \"time\", \"to\", \"two\", \"up\", \"use\",\n",
    "               \"very\", \"want\", \"way\", \"we\", \"well\", \"what\", \"when\", \"which\", \"who\", \"will\",\n",
    "               \"with\", \"would\", \"year\", \"you\", \"your\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix.keys():\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "for word in other_words:\n",
    "    if word not in word_to_ix.keys():\n",
    "        word_to_ix[word] = len(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2, \"ADJ\": 3, \"ADV\": 4, \"PRP\": 5, \"PRN\": 6}\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim).to(device)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size).to(device)\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        sentence.to(device)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        lstm_out.to(device)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_score = F.log_softmax(tag_space, dim = 1)\n",
    "        return tag_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a sample tenset \n",
      " Sentence:\n",
      " everybody read the book and ate the food \n",
      " she like my dog\n",
      "[('everybody', 'ADJ'), ('read', 'ADJ'), ('the', 'V'), ('book', 'NN'), ('and', 'V'), ('ate', 'V'), ('the', 'V'), ('food', 'V')]\n",
      "[('she', 'V'), ('like', 'V'), ('my', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# test a sentence\n",
    "seq1 = \"everybody read the book and ate the food\".split()\n",
    "seq2 = \"she like my dog\".split()\n",
    "print(\"Running a sample tenset \\n Sentence:\\n {} \\n {}\".format(\" \".join(seq1),\n",
    "                                                               \" \".join(seq2)))\n",
    "with torch.no_grad():\n",
    "    for seq in [seq1, seq2]:\n",
    "        model.to(device)\n",
    "        inputs = prepare_sequence(seq, word_to_ix).to(device)\n",
    "        tag_score = model(inputs)\n",
    "        max_indices = tag_score.max(dim=1)[1]\n",
    "        ret = []\n",
    "        # reverse tag_to_ix\n",
    "        reverse_tag_index = {v: k for k, v in tag_to_ix.items()}\n",
    "        for i in range(len(max_indices)):\n",
    "            idx = int(max_indices[i])\n",
    "            ret.append((seq[i], reverse_tag_index[idx]))\n",
    "        print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss 1.9295401573181152\n",
      "Epoch: 1, Loss 1.8933115005493164\n",
      "Epoch: 2, Loss 1.8580784797668457\n",
      "Epoch: 3, Loss 1.8237969875335693\n",
      "Epoch: 4, Loss 1.7892918586730957\n",
      "Epoch: 5, Loss 1.753929615020752\n",
      "Epoch: 6, Loss 1.7171865701675415\n",
      "Epoch: 7, Loss 1.6786421537399292\n",
      "Epoch: 8, Loss 1.6379826068878174\n",
      "Epoch: 9, Loss 1.595015287399292\n",
      "Epoch: 10, Loss 1.5496798753738403\n",
      "Epoch: 11, Loss 1.5020530223846436\n",
      "Epoch: 12, Loss 1.4523417949676514\n",
      "Epoch: 13, Loss 1.4008657932281494\n",
      "Epoch: 14, Loss 1.3480287790298462\n",
      "Epoch: 15, Loss 1.2942864894866943\n",
      "Epoch: 16, Loss 1.2401100397109985\n",
      "Epoch: 17, Loss 1.1859546899795532\n",
      "Epoch: 18, Loss 1.1322342157363892\n",
      "Epoch: 19, Loss 1.0793037414550781\n",
      "Epoch: 20, Loss 1.0274531841278076\n",
      "Epoch: 21, Loss 0.97690749168396\n",
      "Epoch: 22, Loss 0.927833080291748\n",
      "Epoch: 23, Loss 0.8803455233573914\n",
      "Epoch: 24, Loss 0.8345179557800293\n",
      "Epoch: 25, Loss 0.7903887033462524\n",
      "Epoch: 26, Loss 0.747968316078186\n",
      "Epoch: 27, Loss 0.7072464227676392\n",
      "Epoch: 28, Loss 0.668196439743042\n",
      "Epoch: 29, Loss 0.6307816505432129\n",
      "Epoch: 30, Loss 0.5949642658233643\n",
      "Epoch: 31, Loss 0.5607171058654785\n",
      "Epoch: 32, Loss 0.52802973985672\n",
      "Epoch: 33, Loss 0.49689826369285583\n",
      "Epoch: 34, Loss 0.46731019020080566\n",
      "Epoch: 35, Loss 0.43923935294151306\n",
      "Epoch: 36, Loss 0.4126511216163635\n",
      "Epoch: 37, Loss 0.3875080943107605\n",
      "Epoch: 38, Loss 0.3637720048427582\n",
      "Epoch: 39, Loss 0.341403603553772\n",
      "Epoch: 40, Loss 0.32036224007606506\n",
      "Epoch: 41, Loss 0.3006048798561096\n",
      "Epoch: 42, Loss 0.2820858061313629\n",
      "Epoch: 43, Loss 0.2647565007209778\n",
      "Epoch: 44, Loss 0.24856583774089813\n",
      "Epoch: 45, Loss 0.23346099257469177\n",
      "Epoch: 46, Loss 0.21938863396644592\n",
      "Epoch: 47, Loss 0.20629583299160004\n",
      "Epoch: 48, Loss 0.19413048028945923\n",
      "Epoch: 49, Loss 0.1828402280807495\n",
      "Epoch: 50, Loss 0.17237211763858795\n",
      "Epoch: 51, Loss 0.16267231106758118\n",
      "Epoch: 52, Loss 0.15368714928627014\n",
      "Epoch: 53, Loss 0.1453641653060913\n",
      "Epoch: 54, Loss 0.13765248656272888\n",
      "Epoch: 55, Loss 0.13050377368927002\n",
      "Epoch: 56, Loss 0.1238725408911705\n",
      "Epoch: 57, Loss 0.11771637946367264\n",
      "Epoch: 58, Loss 0.1119961217045784\n",
      "Epoch: 59, Loss 0.1066756397485733\n",
      "Epoch: 60, Loss 0.10172189772129059\n",
      "Epoch: 61, Loss 0.09710468351840973\n",
      "Epoch: 62, Loss 0.09279631823301315\n",
      "Epoch: 63, Loss 0.08877173811197281\n",
      "Epoch: 64, Loss 0.08500796556472778\n",
      "Epoch: 65, Loss 0.08148423582315445\n",
      "Epoch: 66, Loss 0.0781814232468605\n",
      "Epoch: 67, Loss 0.07508236169815063\n",
      "Epoch: 68, Loss 0.07217130810022354\n",
      "Epoch: 69, Loss 0.06943383067846298\n",
      "Epoch: 70, Loss 0.0668569803237915\n",
      "Epoch: 71, Loss 0.06442874670028687\n",
      "Epoch: 72, Loss 0.062138259410858154\n",
      "Epoch: 73, Loss 0.05997554957866669\n",
      "Epoch: 74, Loss 0.05793145298957825\n",
      "Epoch: 75, Loss 0.05599763244390488\n",
      "Epoch: 76, Loss 0.05416634678840637\n",
      "Epoch: 77, Loss 0.052430540323257446\n",
      "Epoch: 78, Loss 0.05078372359275818\n",
      "Epoch: 79, Loss 0.04921993240714073\n",
      "Epoch: 80, Loss 0.0477336123585701\n",
      "Epoch: 81, Loss 0.04631970450282097\n",
      "Epoch: 82, Loss 0.0449734702706337\n",
      "Epoch: 83, Loss 0.04369065538048744\n",
      "Epoch: 84, Loss 0.04246722161769867\n",
      "Epoch: 85, Loss 0.04129956290125847\n",
      "Epoch: 86, Loss 0.04018430411815643\n",
      "Epoch: 87, Loss 0.03911833092570305\n",
      "Epoch: 88, Loss 0.038098882883787155\n",
      "Epoch: 89, Loss 0.037123311311006546\n",
      "Epoch: 90, Loss 0.036189280450344086\n",
      "Epoch: 91, Loss 0.03529447689652443\n",
      "Epoch: 92, Loss 0.034436848014593124\n",
      "Epoch: 93, Loss 0.033614419400691986\n",
      "Epoch: 94, Loss 0.03282540664076805\n",
      "Epoch: 95, Loss 0.032067958265542984\n",
      "Epoch: 96, Loss 0.031340453773736954\n",
      "Epoch: 97, Loss 0.030641263350844383\n",
      "Epoch: 98, Loss 0.02996901609003544\n",
      "Epoch: 99, Loss 0.02932221069931984\n",
      "Epoch: 100, Loss 0.02869952842593193\n",
      "Epoch: 101, Loss 0.028099749237298965\n",
      "Epoch: 102, Loss 0.027521712705492973\n",
      "Epoch: 103, Loss 0.02696429193019867\n",
      "Epoch: 104, Loss 0.026426497846841812\n",
      "Epoch: 105, Loss 0.025907428935170174\n",
      "Epoch: 106, Loss 0.025406120344996452\n",
      "Epoch: 107, Loss 0.02492174506187439\n",
      "Epoch: 108, Loss 0.024453554302453995\n",
      "Epoch: 109, Loss 0.02400081232190132\n",
      "Epoch: 110, Loss 0.023562807589769363\n",
      "Epoch: 111, Loss 0.023138880729675293\n",
      "Epoch: 112, Loss 0.022728418931365013\n",
      "Epoch: 113, Loss 0.022330882027745247\n",
      "Epoch: 114, Loss 0.02194564789533615\n",
      "Epoch: 115, Loss 0.021572206169366837\n",
      "Epoch: 116, Loss 0.021210134029388428\n",
      "Epoch: 117, Loss 0.02085888758301735\n",
      "Epoch: 118, Loss 0.020518051460385323\n",
      "Epoch: 119, Loss 0.020187200978398323\n",
      "Epoch: 120, Loss 0.01986595243215561\n",
      "Epoch: 121, Loss 0.01955389231443405\n",
      "Epoch: 122, Loss 0.01925070211291313\n",
      "Epoch: 123, Loss 0.018955988809466362\n",
      "Epoch: 124, Loss 0.018669506534934044\n",
      "Epoch: 125, Loss 0.018390916287899017\n",
      "Epoch: 126, Loss 0.01811983436346054\n",
      "Epoch: 127, Loss 0.017856111750006676\n",
      "Epoch: 128, Loss 0.017599433660507202\n",
      "Epoch: 129, Loss 0.017349526286125183\n",
      "Epoch: 130, Loss 0.0171060673892498\n",
      "Epoch: 131, Loss 0.016868960112333298\n",
      "Epoch: 132, Loss 0.01663791947066784\n",
      "Epoch: 133, Loss 0.01641271822154522\n",
      "Epoch: 134, Loss 0.016193127259612083\n",
      "Epoch: 135, Loss 0.015979008749127388\n",
      "Epoch: 136, Loss 0.01577012427151203\n",
      "Epoch: 137, Loss 0.015566302463412285\n",
      "Epoch: 138, Loss 0.015367326326668262\n",
      "Epoch: 139, Loss 0.01517309620976448\n",
      "Epoch: 140, Loss 0.014983381144702435\n",
      "Epoch: 141, Loss 0.01479803305119276\n",
      "Epoch: 142, Loss 0.014616969972848892\n",
      "Epoch: 143, Loss 0.014439934864640236\n",
      "Epoch: 144, Loss 0.014266902580857277\n",
      "Epoch: 145, Loss 0.014097646810114384\n",
      "Epoch: 146, Loss 0.013932077214121819\n",
      "Epoch: 147, Loss 0.01377007458359003\n",
      "Epoch: 148, Loss 0.01361154392361641\n",
      "Epoch: 149, Loss 0.01345633901655674\n",
      "Epoch: 150, Loss 0.013304331339895725\n",
      "Epoch: 151, Loss 0.0131554389372468\n",
      "Epoch: 152, Loss 0.013009589165449142\n",
      "Epoch: 153, Loss 0.012866721488535404\n",
      "Epoch: 154, Loss 0.012726620770990849\n",
      "Epoch: 155, Loss 0.012589341029524803\n",
      "Epoch: 156, Loss 0.012454695999622345\n",
      "Epoch: 157, Loss 0.012322686612606049\n",
      "Epoch: 158, Loss 0.012193169444799423\n",
      "Epoch: 159, Loss 0.012066126801073551\n",
      "Epoch: 160, Loss 0.011941458098590374\n",
      "Epoch: 161, Loss 0.0118191447108984\n",
      "Epoch: 162, Loss 0.011699066497385502\n",
      "Epoch: 163, Loss 0.01158120110630989\n",
      "Epoch: 164, Loss 0.011465477757155895\n",
      "Epoch: 165, Loss 0.011351893655955791\n",
      "Epoch: 166, Loss 0.011240311898291111\n",
      "Epoch: 167, Loss 0.011130713857710361\n",
      "Epoch: 168, Loss 0.011023084633052349\n",
      "Epoch: 169, Loss 0.010917319916188717\n",
      "Epoch: 170, Loss 0.010813402011990547\n",
      "Epoch: 171, Loss 0.010711328126490116\n",
      "Epoch: 172, Loss 0.010610993951559067\n",
      "Epoch: 173, Loss 0.010512364096939564\n",
      "Epoch: 174, Loss 0.010415409691631794\n",
      "Epoch: 175, Loss 0.01032011304050684\n",
      "Epoch: 176, Loss 0.010226396843791008\n",
      "Epoch: 177, Loss 0.010134211741387844\n",
      "Epoch: 178, Loss 0.010043591260910034\n",
      "Epoch: 179, Loss 0.0099543621763587\n",
      "Epoch: 180, Loss 0.009866624139249325\n",
      "Epoch: 181, Loss 0.009780237451195717\n",
      "Epoch: 182, Loss 0.009695242159068584\n",
      "Epoch: 183, Loss 0.009611538611352444\n",
      "Epoch: 184, Loss 0.009529094211757183\n",
      "Epoch: 185, Loss 0.009447959251701832\n",
      "Epoch: 186, Loss 0.00936795212328434\n",
      "Epoch: 187, Loss 0.009289130568504333\n",
      "Epoch: 188, Loss 0.009211476892232895\n",
      "Epoch: 189, Loss 0.00913489144295454\n",
      "Epoch: 190, Loss 0.009059405885636806\n",
      "Epoch: 191, Loss 0.008984990417957306\n",
      "Epoch: 192, Loss 0.008911591954529285\n",
      "Epoch: 193, Loss 0.00883912667632103\n",
      "Epoch: 194, Loss 0.008767672814428806\n",
      "Epoch: 195, Loss 0.008697143755853176\n",
      "Epoch: 196, Loss 0.00862758606672287\n",
      "Epoch: 197, Loss 0.008558898232877254\n",
      "Epoch: 198, Loss 0.00849108211696148\n",
      "Epoch: 199, Loss 0.008424167521297932\n",
      "Epoch: 200, Loss 0.008358094841241837\n",
      "Epoch: 201, Loss 0.008292849175632\n",
      "Epoch: 202, Loss 0.008228452876210213\n",
      "Epoch: 203, Loss 0.008164883591234684\n",
      "Epoch: 204, Loss 0.008102096617221832\n",
      "Epoch: 205, Loss 0.00804015714675188\n",
      "Epoch: 206, Loss 0.007978979498147964\n",
      "Epoch: 207, Loss 0.007918624207377434\n",
      "Epoch: 208, Loss 0.007859052158892155\n",
      "Epoch: 209, Loss 0.00780025590211153\n",
      "Epoch: 210, Loss 0.007742264773696661\n",
      "Epoch: 211, Loss 0.007685039192438126\n",
      "Epoch: 212, Loss 0.0076285419054329395\n",
      "Epoch: 213, Loss 0.007572806440293789\n",
      "Epoch: 214, Loss 0.007517829537391663\n",
      "Epoch: 215, Loss 0.007463572081178427\n",
      "Epoch: 216, Loss 0.007410045247524977\n",
      "Epoch: 217, Loss 0.007357223890721798\n",
      "Epoch: 218, Loss 0.007305145263671875\n",
      "Epoch: 219, Loss 0.0072537316009402275\n",
      "Epoch: 220, Loss 0.007203035056591034\n",
      "Epoch: 221, Loss 0.007152988575398922\n",
      "Epoch: 222, Loss 0.0071035828441381454\n",
      "Epoch: 223, Loss 0.007054853718727827\n",
      "Epoch: 224, Loss 0.007006751373410225\n",
      "Epoch: 225, Loss 0.006959289312362671\n",
      "Epoch: 226, Loss 0.006912474054843187\n",
      "Epoch: 227, Loss 0.006866225507110357\n",
      "Epoch: 228, Loss 0.006820586510002613\n",
      "Epoch: 229, Loss 0.006775529123842716\n",
      "Epoch: 230, Loss 0.006731040775775909\n",
      "Epoch: 231, Loss 0.006687110289931297\n",
      "Epoch: 232, Loss 0.006643753964453936\n",
      "Epoch: 233, Loss 0.006600906141102314\n",
      "Epoch: 234, Loss 0.006558635272085667\n",
      "Epoch: 235, Loss 0.006516852881759405\n",
      "Epoch: 236, Loss 0.006475594360381365\n",
      "Epoch: 237, Loss 0.0064348625019192696\n",
      "Epoch: 238, Loss 0.006394580937922001\n",
      "Epoch: 239, Loss 0.0063548325560987\n",
      "Epoch: 240, Loss 0.0063155172392725945\n",
      "Epoch: 241, Loss 0.006276692729443312\n",
      "Epoch: 242, Loss 0.0062383245676755905\n",
      "Epoch: 243, Loss 0.006200444884598255\n",
      "Epoch: 244, Loss 0.006162973586469889\n",
      "Epoch: 245, Loss 0.0061259157955646515\n",
      "Epoch: 246, Loss 0.006089311093091965\n",
      "Epoch: 247, Loss 0.006053129211068153\n",
      "Epoch: 248, Loss 0.006017348263412714\n",
      "Epoch: 249, Loss 0.0059819878078997135\n",
      "Epoch: 250, Loss 0.00594702735543251\n",
      "Epoch: 251, Loss 0.005912461318075657\n",
      "Epoch: 252, Loss 0.0058782659471035\n",
      "Epoch: 253, Loss 0.005844451487064362\n",
      "Epoch: 254, Loss 0.0058109816163778305\n",
      "Epoch: 255, Loss 0.0057779042981565\n",
      "Epoch: 256, Loss 0.0057452041655778885\n",
      "Epoch: 257, Loss 0.005712817423045635\n",
      "Epoch: 258, Loss 0.005680786445736885\n",
      "Epoch: 259, Loss 0.005649095866829157\n",
      "Epoch: 260, Loss 0.005617744754999876\n",
      "Epoch: 261, Loss 0.005586746148765087\n",
      "Epoch: 262, Loss 0.005556019954383373\n",
      "Epoch: 263, Loss 0.005525649059563875\n",
      "Epoch: 264, Loss 0.00549557339400053\n",
      "Epoch: 265, Loss 0.0054657976143062115\n",
      "Epoch: 266, Loss 0.005436293315142393\n",
      "Epoch: 267, Loss 0.005407159682363272\n",
      "Epoch: 268, Loss 0.005378264002501965\n",
      "Epoch: 269, Loss 0.005349671468138695\n",
      "Epoch: 270, Loss 0.005321330390870571\n",
      "Epoch: 271, Loss 0.005293304566293955\n",
      "Epoch: 272, Loss 0.0052654929459095\n",
      "Epoch: 273, Loss 0.005238036625087261\n",
      "Epoch: 274, Loss 0.005210777744650841\n",
      "Epoch: 275, Loss 0.005183788947761059\n",
      "Epoch: 276, Loss 0.005157046020030975\n",
      "Epoch: 277, Loss 0.00513054383918643\n",
      "Epoch: 278, Loss 0.005104312673211098\n",
      "Epoch: 279, Loss 0.005078290123492479\n",
      "Epoch: 280, Loss 0.005052503664046526\n",
      "Epoch: 281, Loss 0.005026936996728182\n",
      "Epoch: 282, Loss 0.005001578014343977\n",
      "Epoch: 283, Loss 0.004976458381861448\n",
      "Epoch: 284, Loss 0.004951538983732462\n",
      "Epoch: 285, Loss 0.004926785826683044\n",
      "Epoch: 286, Loss 0.004902239423245192\n",
      "Epoch: 287, Loss 0.004877910949289799\n",
      "Epoch: 288, Loss 0.0048537477850914\n",
      "Epoch: 289, Loss 0.004829730372875929\n",
      "Epoch: 290, Loss 0.004805864300578833\n",
      "Epoch: 291, Loss 0.0047821965999901295\n",
      "Epoch: 292, Loss 0.004758596420288086\n",
      "Epoch: 293, Loss 0.004735129419714212\n",
      "Epoch: 294, Loss 0.004711808171123266\n",
      "Epoch: 295, Loss 0.004688580520451069\n",
      "Epoch: 296, Loss 0.004665540065616369\n",
      "Epoch: 297, Loss 0.00464263278990984\n",
      "Epoch: 298, Loss 0.004619977902621031\n",
      "Epoch: 299, Loss 0.004597574472427368\n",
      "Train Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "losses = []\n",
    "model.to(device)\n",
    "for epoch in range(300):\n",
    "    count = 0\n",
    "    sum_loss = 0\n",
    "    for sentence, tags in training_data:\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix).to(device)\n",
    "        targets = prepare_sequence(tags, tag_to_ix).to(device)\n",
    "        out = model(sentence_in)\n",
    "        loss = loss_function(out, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        count += 1\n",
    "        sum_loss += loss\n",
    "        losses.append(sum_loss / count)\n",
    "    print(\"Epoch: {}, Loss {}\".format(epoch, losses[-1]))\n",
    "print(\"Train Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# predict function\n",
    "def predict_seq(seq_list, model):\n",
    "    \"\"\"\n",
    "\n",
    "    :param seq_list: list of sequences\n",
    "    :param model: NN model\n",
    "    :return: tuple predictions\n",
    "    \"\"\"\n",
    "    # model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for seq in seq_list:\n",
    "            inputs = prepare_sequence(seq, word_to_ix).to(device)\n",
    "            tags_score = model(inputs)\n",
    "            max_indices = tags_score.max(dim=1)[1]\n",
    "            pred = []\n",
    "            reverse_tag_index = {v: k for k, v in tag_to_ix.items()}\n",
    "            for i in range(len(max_indices)):\n",
    "                idx = int(max_indices[i])\n",
    "                pred.append(reverse_tag_index[idx])\n",
    "            print(\"Sequence: {} \\n\"\n",
    "              \"Tag Prediction: {}\\n\".format(seq, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: ['everybody', 'read', 'the', 'book', 'and', 'ate', 'the', 'food'] \n",
      "Tag Prediction: ['NN', 'V', 'DET', 'NN', 'NN', 'V', 'DET', 'ADJ']\n",
      "\n",
      "Sequence: ['she', 'like', 'my', 'dog'] \n",
      "Tag Prediction: ['PRN', 'V', 'PRN', 'NN']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test on unkown data\n",
    "predict_seq([seq1, seq2], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Read in\n",
    "[back to top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_text(text_file, by_line=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param by_line: bool, whether to split by lines; if False, split by word\n",
    "    :param text_file: training file\n",
    "    :return: DIC, TOKENS and TAGS\n",
    "\n",
    "    \"\"\"\n",
    "    if by_line == False:\n",
    "        with open(text_file, mode=\"r\") as file:\n",
    "            text_f = file.read()\n",
    "            text_f_lst = text_f.split()\n",
    "            file.close()\n",
    "        keys, values = text_f_lst[::2], text_f_lst[1::2]\n",
    "        result_dic = dict(zip(keys, values))\n",
    "        return result_dic, keys, values\n",
    "    else:\n",
    "        with open(text_file, mode=\"r\") as file:\n",
    "            text_f = file.read()\n",
    "            text_f_lst = text_f.splitlines()\n",
    "            file.close()\n",
    "        keys = [line.split()[::2] for line in text_f_lst]\n",
    "        values = [line.split()[1::2] for line in text_f_lst]\n",
    "        # result_dic = dict(zip(keys, values))\n",
    "        return keys, values\n",
    "# create a list of list of tuples for training data\n",
    "def combine_lists(vocab_list, tags_list):\n",
    "    \"\"\"\n",
    "\n",
    "    :param vocab_list: list of sentence\n",
    "    :param tags_list:\n",
    "    :return: list of list of sentence of words tuples e.g. [[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.')]]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(len(vocab_list)):\n",
    "        sentence, tags = vocab_list[i], tags_list[i]\n",
    "        zipped = zip(sentence, tags)\n",
    "        result.append(list(zipped))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_list, tags_list = split_text(\"wsj1-18.training\", by_line=True)\n",
    "train_list = combine_lists(vocab_list, tags_list)\n",
    "test_vocab_list, test_tags_list = split_text(\"wsj19-21.truth\", by_line=True)\n",
    "test_list = combine_lists(test_vocab_list, test_tags_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Construct dictionary\n",
    "1. A word/tag dictionary\n",
    "2. A letter/character dictionary\n",
    "3. A POS tag dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sequence_to_idx(words, dic_ix):\n",
    "    \"\"\"\n",
    "\n",
    "    :param words: list of words\n",
    "    :param dic_ix: dictionary with the index as values, word as keys\n",
    "    :return: list of indices\n",
    "    \"\"\"\n",
    "    return torch.tensor([dic_ix[word] for word in words], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 46620\n",
      "Unique tags: 45\n",
      "Unique characters: 80\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {}\n",
    "tag_to_idx = {}\n",
    "char_to_idx = {}\n",
    "for sentence in train_list:\n",
    "    for word, tag in sentence:\n",
    "        if word not in word_to_idx.keys():\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "        if tag not in tag_to_idx.keys():\n",
    "            tag_to_idx[tag] = len(tag_to_idx)\n",
    "        for char in word:\n",
    "            if char not in char_to_idx.keys():\n",
    "                char_to_idx[char] = len(char_to_idx)\n",
    "word_vocab_size = len(word_to_idx)\n",
    "tag_vocab_size = len(tag_to_idx)\n",
    "char_vocab_size = len(char_to_idx)\n",
    "for sentence in test_vocab_list:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_idx.keys():\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "print(\"Unique words: {}\".format(len(word_to_idx)))\n",
    "print(\"Unique tags: {}\".format(len(tag_to_idx)))\n",
    "print(\"Unique characters: {}\".format(len(char_to_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Specify hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model, if_train):\n",
    "    if if_train:\n",
    "        data = list(zip(vocab_list, tags_list))\n",
    "    else:\n",
    "        data = list(zip(test_vocab_list, test_tags_list))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for input_tuple in data:\n",
    "            # get X in a list by unzipping the input tuple list\n",
    "            X = input_tuple[0]\n",
    "            # get Y similarly\n",
    "            y = input_tuple[1]\n",
    "            # convert into index\n",
    "            X = prepare_sequence(X, word_to_idx).to(device)\n",
    "            y = prepare_sequence(y, tag_to_idx).to(device)\n",
    "            # forward model\n",
    "            out = model(X)\n",
    "            max_indices = out.max(dim=1)[1]\n",
    "            total += len(y)\n",
    "            # because prepare sequence output long type tensor\n",
    "            correct = torch.eq(max_indices, y).sum().item()\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "WORD_EMBEDDING_DIM = 1024\n",
    "CHAR_EMBEDDING_DIM = 128\n",
    "WORD_HIDDEN_DIM = 1024\n",
    "CHAR_HIDDEN_DIM = 1024\n",
    "EPOCHS = 100\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "debug = True\n",
    "def train(model, lr, epochs=EPOCHS):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    data = list(zip(vocab_list, tags_list))\n",
    "    # init losses,\n",
    "    losses, train_acc, test_acc = [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        iters = 0\n",
    "        sum_loss = 0\n",
    "        for batch_id, (X, y) in enumerate(data):\n",
    "            if debug: print('Beginning Reading Batch: {}'.format(batch_id))\n",
    "            X = prepare_sequence(X, word_to_idx).to(device)\n",
    "            y = prepare_sequence(y, tag_to_idx).to(device)\n",
    "            model.to(device)\n",
    "            # forward model pass\n",
    "            out = model(X)\n",
    "            loss = loss_function(out, y) # compute the loss\n",
    "            loss.backward() # backward pass\n",
    "            optimizer.step() # make the update to each parameter\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # save result\n",
    "            if debug: print('Calculated Loss')\n",
    "            sum_loss += loss\n",
    "            iters += 1\n",
    "            train_acc.append(get_accuracy(model, True))\n",
    "            if debug: print('Calculated Train Acc')\n",
    "            test_acc.append(get_accuracy(model, False))\n",
    "            if debug: print('Calculated Test Acc')\n",
    "            losses.append(sum_loss / iters)\n",
    "            # if batch_id % 100 == 0:\n",
    "            print(\"Epoch: {}, Batch: {} \\n\"\n",
    "                      \"Loss{}, Train Accuracy{:.2%}, Test Accuracy:{:.2%}\".format(\n",
    "                    epoch, batch_id, losses[-1], train_acc[-1], test_acc[-1]\n",
    "                ) )\n",
    "        # plotting\n",
    "        plt.title(\"Training Curve\")\n",
    "        plt.plot(np.arange(len(losses)), losses, label=\"Loss\")\n",
    "        plt.plot(np.arange(len(losses)), train_acc, linestyle='-.', label=\"Train\")\n",
    "        plt.plot(np.arange(len(losses)), test_acc, linestyle='-.', label=\"Test\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss/ Accuracy\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "    return model\n",
    "    print(\"Finished Training\\n\" + \"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m LSTMTagger(WORD_EMBEDDING_DIM, WORD_HIDDEN_DIM, word_vocab_size, tag_vocab_size)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mget_accuracy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[22], line 21\u001B[0m, in \u001B[0;36mget_accuracy\u001B[0;34m(model, if_train)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# forward model\u001B[39;00m\n\u001B[1;32m     20\u001B[0m out \u001B[38;5;241m=\u001B[39m model(X)\n\u001B[0;32m---> 21\u001B[0m max_indices \u001B[38;5;241m=\u001B[39m \u001B[43mout\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     22\u001B[0m total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(y)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# because prepare sequence output long type tensor\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(WORD_EMBEDDING_DIM, WORD_HIDDEN_DIM, word_vocab_size, tag_vocab_size)\n",
    "get_accuracy(model, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "out = torch.tensor([[-3.8064, -3.6861, -3.8779, -3.8170, -3.8165, -3.8231, -3.7911, -3.7696,\n",
    "         -3.7775, -3.7533, -3.8098, -3.7418, -3.6407, -3.8113, -3.8538, -3.9267,\n",
    "         -3.8926, -3.7769, -3.9160, -3.7349, -3.8407, -3.8920, -3.8366, -3.8859,\n",
    "         -3.8680, -3.8010, -3.8925, -3.7323, -3.8046, -3.8256, -3.8375, -3.7195,\n",
    "         -3.8401, -3.8339, -3.8228, -3.7650, -3.6468, -3.9694, -3.7715, -3.7675,\n",
    "         -3.7508, -3.8195, -3.7975, -3.8476, -3.8106],\n",
    "        [-3.8204, -3.8007, -3.8056, -3.8045, -3.7528, -3.8282, -3.8481, -3.9125,\n",
    "         -3.7190, -3.7485, -3.7960, -3.8217, -3.6641, -3.9218, -3.8684, -3.8554,\n",
    "         -3.7569, -3.8234, -3.7805, -3.7599, -3.7906, -3.8922, -3.7402, -3.8611,\n",
    "         -3.7926, -3.8411, -3.8018, -3.8120, -3.7535, -3.8739, -3.7824, -3.7007,\n",
    "         -3.6913, -3.9396, -3.8181, -3.8038, -3.7777, -3.7581, -3.8490, -3.9085,\n",
    "         -3.7803, -3.7878, -3.7937, -3.8650, -3.8788],\n",
    "        [-3.7569, -3.6763, -3.7374, -3.8051, -3.8043, -3.9225, -3.9404, -3.7755,\n",
    "         -3.8280, -3.8150, -3.8818, -3.7948, -3.7637, -3.9012, -3.8660, -3.8266,\n",
    "         -3.6830, -3.8665, -3.8371, -3.7036, -3.7418, -3.8788, -3.8114, -3.8174,\n",
    "         -3.8085, -3.7919, -3.9459, -3.8970, -3.7160, -3.8466, -3.7621, -3.6743,\n",
    "         -3.8311, -3.7939, -3.8235, -3.8004, -3.8518, -3.7786, -3.9431, -3.7774,\n",
    "         -3.7599, -3.8003, -3.7347, -3.8027, -3.8277],\n",
    "        [-3.7147, -3.8125, -3.8317, -3.8253, -3.8458, -3.8643, -4.0009, -3.8040,\n",
    "         -3.6672, -3.7972, -3.8258, -3.7802, -3.6552, -3.8383, -3.8643, -3.8381,\n",
    "         -3.8039, -3.7895, -3.7981, -3.6961, -3.8237, -3.8914, -3.8287, -3.8331,\n",
    "         -3.8750, -3.8002, -3.9484, -3.7619, -3.7871, -3.8754, -3.6865, -3.8059,\n",
    "         -3.8233, -3.7737, -3.7274, -3.6838, -3.9293, -3.6790, -4.0250, -3.9600,\n",
    "         -3.7300, -3.8410, -3.6889, -3.8822, -3.7465],\n",
    "        [-3.6645, -3.7854, -3.9518, -3.8959, -3.9094, -3.8217, -3.9750, -3.8862,\n",
    "         -3.7229, -3.7642, -3.8015, -3.8544, -3.7993, -3.8259, -3.8096, -3.7836,\n",
    "         -3.7416, -3.7645, -3.6310, -3.7750, -3.8153, -3.9916, -3.9341, -3.8078,\n",
    "         -3.7710, -3.8773, -3.8675, -3.7755, -3.8245, -3.7448, -3.6529, -3.8821,\n",
    "         -3.8041, -3.7302, -3.7981, -3.6997, -3.7841, -3.8125, -3.8720, -3.8339,\n",
    "         -3.8696, -3.8532, -3.6832, -3.7968, -3.7966],\n",
    "        [-3.6956, -3.8468, -3.9124, -3.8209, -4.0121, -3.8150, -3.9132, -3.8701,\n",
    "         -3.7798, -3.8592, -3.8821, -3.9241, -3.6536, -3.9529, -3.8873, -3.7850,\n",
    "         -3.8297, -3.7944, -3.6789, -3.8263, -3.7308, -4.0436, -3.9455, -3.7647,\n",
    "         -3.7955, -3.8426, -3.9011, -3.8362, -3.7854, -3.6361, -3.6591, -3.8751,\n",
    "         -3.7482, -3.7232, -3.7242, -3.7383, -3.7086, -3.8622, -3.8436, -3.8506,\n",
    "         -3.7065, -3.7771, -3.6798, -3.7712, -3.8052],\n",
    "        [-3.6481, -3.6580, -3.8551, -3.8303, -3.9002, -3.9156, -3.9515, -3.7677,\n",
    "         -3.8439, -3.8388, -3.9169, -3.8357, -3.7593, -3.9459, -3.8980, -3.7742,\n",
    "         -3.7654, -3.8263, -3.7763, -3.7291, -3.7382, -3.9384, -3.9579, -3.7597,\n",
    "         -3.7772, -3.7760, -3.9407, -3.9049, -3.7665, -3.7555, -3.6619, -3.7553,\n",
    "         -3.8602, -3.6737, -3.8073, -3.7668, -3.8015, -3.8503, -3.9475, -3.7952,\n",
    "         -3.7017, -3.8189, -3.7468, -3.7496, -3.7689],\n",
    "        [-3.7404, -3.7937, -3.7227, -3.8239, -3.6854, -3.7936, -3.9346, -3.8563,\n",
    "         -3.7726, -3.9204, -3.8403, -3.9875, -3.7404, -3.9096, -3.8798, -3.7911,\n",
    "         -3.8267, -3.8006, -3.6606, -3.7481, -3.8127, -3.8676, -3.8146, -3.7009,\n",
    "         -3.9097, -3.7640, -3.8747, -3.8817, -3.8971, -3.7301, -3.7155, -3.7230,\n",
    "         -3.7566, -3.7933, -3.7800, -3.7091, -3.8970, -3.8651, -3.9053, -3.8382,\n",
    "         -3.7105, -3.7954, -3.7717, -3.9200, -3.7702],\n",
    "        [-3.8033, -3.8047, -3.7593, -3.8153, -3.8882, -3.8669, -3.8223, -3.8760,\n",
    "         -3.6245, -3.9543, -3.7404, -3.8470, -3.8600, -3.7891, -3.8045, -3.6943,\n",
    "         -3.8185, -3.8256, -3.7910, -3.6924, -3.8688, -3.8127, -3.8309, -3.7382,\n",
    "         -3.9271, -3.7385, -3.7064, -3.7792, -3.9335, -3.6776, -3.6746, -3.7578,\n",
    "         -3.8621, -3.9000, -3.8528, -3.6432, -3.9483, -4.0025, -3.8727, -3.7270,\n",
    "         -3.8257, -3.8437, -3.7421, -4.0123, -3.7257],\n",
    "        [-3.7449, -3.7040, -3.8578, -3.7934, -3.7758, -3.8208, -3.8068, -3.7229,\n",
    "         -3.7574, -3.8762, -3.7685, -3.9012, -3.7643, -3.8826, -3.8437, -3.7367,\n",
    "         -3.6822, -3.8863, -3.8725, -3.7964, -3.8735, -3.8933, -3.9225, -3.7333,\n",
    "         -4.0712, -3.8093, -3.8181, -3.6794, -3.8298, -3.7567, -3.6711, -3.7937,\n",
    "         -3.8066, -3.8319, -3.7601, -3.6804, -3.8582, -4.0918, -3.8298, -3.7881,\n",
    "         -3.7601, -3.7846, -3.8611, -3.8545, -3.7122],\n",
    "        [-3.7795, -3.7394, -3.9347, -3.8260, -3.8177, -3.7251, -3.8236, -3.7337,\n",
    "         -3.8545, -3.8107, -3.7864, -3.8783, -3.7562, -3.7930, -3.9473, -3.7280,\n",
    "         -3.6568, -3.8576, -3.9113, -3.7663, -3.7479, -3.8081, -3.8670, -3.7979,\n",
    "         -3.8608, -3.8325, -3.8843, -3.7879, -3.8240, -3.8226, -3.7083, -3.7941,\n",
    "         -3.9223, -3.7505, -3.8664, -3.8225, -3.7289, -3.9755, -3.8062, -3.8003,\n",
    "         -3.8000, -3.8345, -3.6527, -3.8147, -3.7720],\n",
    "        [-3.6566, -3.7714, -3.8988, -3.8508, -3.8750, -3.8751, -3.9067, -3.8476,\n",
    "         -3.7487, -3.7690, -3.8397, -3.8253, -3.8756, -3.7180, -3.7534, -3.8108,\n",
    "         -3.6753, -3.7920, -3.9032, -3.9020, -3.8138, -3.8956, -3.7094, -3.7127,\n",
    "         -3.9103, -3.7797, -3.7701, -3.7391, -3.8951, -3.7262, -3.7236, -3.7737,\n",
    "         -3.8763, -3.6681, -3.8167, -3.8037, -3.8032, -3.8306, -3.8149, -3.8153,\n",
    "         -3.8833, -3.8267, -3.7720, -3.8554, -3.9012],\n",
    "        [-3.7675, -3.7181, -3.8743, -3.9147, -3.7952, -3.8267, -3.9955, -3.8812,\n",
    "         -3.6174, -3.7832, -3.9333, -3.7849, -3.9526, -3.8020, -3.7493, -3.7639,\n",
    "         -3.7449, -3.8440, -3.8255, -3.8040, -3.7161, -3.8180, -3.8691, -3.8230,\n",
    "         -3.7870, -3.8424, -3.7560, -3.8753, -3.8678, -3.8473, -3.6614, -3.7385,\n",
    "         -3.7661, -3.7236, -3.8554, -3.8372, -3.7889, -3.8353, -3.8465, -3.6777,\n",
    "         -3.7744, -3.8650, -3.7410, -3.8450, -3.8904],\n",
    "        [-3.7336, -3.8178, -3.8059, -3.9887, -3.8826, -3.8109, -3.8448, -3.7167,\n",
    "         -3.7988, -3.9344, -3.8267, -3.7917, -3.8272, -3.8494, -3.8686, -3.6894,\n",
    "         -3.6813, -3.8832, -3.8325, -3.9047, -3.7495, -3.8521, -3.7719, -3.8040,\n",
    "         -3.7823, -3.8780, -3.6638, -3.8351, -3.9140, -3.9574, -3.8628, -3.6902,\n",
    "         -3.6947, -3.8066, -3.8184, -3.7221, -3.7508, -3.8395, -3.8276, -3.8152,\n",
    "         -3.6704, -3.8418, -3.6486, -4.0034, -3.7684],\n",
    "        [-3.6428, -3.6903, -3.8872, -3.9267, -3.9637, -3.9185, -3.8537, -3.6970,\n",
    "         -3.9063, -3.8867, -3.9348, -3.8812, -3.7554, -3.9000, -3.8211, -3.6883,\n",
    "         -3.8609, -3.7764, -3.8229, -3.8394, -3.6786, -3.7303, -3.8804, -3.8648,\n",
    "         -3.7171, -3.9098, -3.8103, -3.6931, -3.8557, -3.8613, -3.8200, -3.6633,\n",
    "         -3.6903, -3.7636, -3.8420, -3.8298, -3.7286, -3.8714, -3.8928, -3.8384,\n",
    "         -3.6701, -3.9039, -3.6694, -3.8458, -3.7947],\n",
    "        [-3.7400, -3.7414, -3.7947, -3.9006, -3.8343, -3.9172, -3.8796, -3.7751,\n",
    "         -3.6915, -3.7453, -3.8106, -3.9302, -3.8431, -3.8374, -3.8393, -3.8796,\n",
    "         -3.8330, -3.7775, -3.8518, -3.8253, -3.7622, -3.7353, -3.9290, -3.8986,\n",
    "         -3.8297, -3.8554, -3.7488, -3.7354, -3.8118, -3.7259, -3.8567, -3.7481,\n",
    "         -3.7224, -3.8127, -3.8633, -3.8826, -3.8303, -3.8331, -3.8073, -3.7549,\n",
    "         -3.6395, -3.8839, -3.7573, -3.7916, -3.7359],\n",
    "        [-3.8293, -3.7964, -3.8912, -3.7737, -3.8711, -3.8490, -3.9733, -3.7915,\n",
    "         -3.7494, -3.8033, -3.6459, -3.9215, -3.8275, -3.7582, -3.7578, -3.9423,\n",
    "         -3.8585, -3.7128, -3.8380, -3.8400, -3.7046, -3.6899, -3.9365, -3.8141,\n",
    "         -3.9000, -3.8374, -3.8161, -3.7728, -3.7878, -3.7544, -3.8721, -3.7127,\n",
    "         -3.7017, -3.8900, -3.8251, -3.8659, -3.6874, -3.8354, -3.8168, -3.7842,\n",
    "         -3.7558, -3.9144, -3.7905, -3.8085, -3.7198],\n",
    "        [-3.9430, -3.6644, -3.7695, -3.8337, -3.8817, -3.8692, -3.9832, -3.7952,\n",
    "         -3.6338, -3.7847, -3.6693, -3.8395, -3.8337, -3.7766, -3.8948, -3.9011,\n",
    "         -3.9402, -3.7512, -3.8840, -3.7673, -3.7424, -3.6445, -3.9830, -3.8825,\n",
    "         -3.9790, -3.9505, -3.9185, -3.7473, -3.7039, -3.7165, -3.8429, -3.7408,\n",
    "         -3.7329, -3.8213, -3.7644, -3.8727, -3.7670, -3.7403, -3.8827, -3.6653,\n",
    "         -3.8695, -3.7374, -3.7576, -3.9057, -3.7185]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([12, 12, 31, 12, 18, 29,  0, 18,  8, 30, 42,  0,  8, 42,  0, 40, 10,  8])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.exp().max(dim=1)[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([12, 12, 31, 12, 18, 29,  0, 18,  8, 30, 42,  0,  8, 42,  0, 40, 10,  8])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.max(dim=1)[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.0222, 0.0251, 0.0207, 0.0220, 0.0220, 0.0219, 0.0226, 0.0231, 0.0229,\n        0.0234, 0.0222, 0.0237, 0.0262, 0.0221, 0.0212, 0.0197, 0.0204, 0.0229,\n        0.0199, 0.0239, 0.0215, 0.0204, 0.0216, 0.0205, 0.0209, 0.0223, 0.0204,\n        0.0239, 0.0223, 0.0218, 0.0215, 0.0242, 0.0215, 0.0216, 0.0219, 0.0232,\n        0.0261, 0.0189, 0.0230, 0.0231, 0.0235, 0.0219, 0.0224, 0.0213, 0.0221])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.exp()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}