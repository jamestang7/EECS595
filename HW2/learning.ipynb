{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Table of Contents\n",
    "1. [Imports](#Imports)\n",
    "2. [Data Read In](#Data-Read-in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports\n",
    "[back to top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import unittest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.has_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Part 1\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"Input: takes in a list of words, and a dictionary containing the index of the words\n",
    "    Output: a tensor containing the indexes of the word\"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "# This is the example training data\n",
    "training_data = [\n",
    "    (\"the dog happily ate the big apple\".split(), [\"DET\", \"NN\", \"ADV\", \"V\", \"DET\", \"ADJ\", \"NN\"]),\n",
    "    (\"everybody read that good book quietly in the hall\".split(), [\"NN\", \"V\", \"DET\", \"ADJ\", \"NN\", \"ADV\", \"PRP\", \"DET\", \"NN\"]),\n",
    "    (\"the old head master sternly scolded the naughty children for \\\n",
    "     being very loud\".split(), [\"DET\", \"ADJ\", \"ADJ\", \"NN\", \"ADV\", \"V\", \"DET\", \"ADJ\",  \"NN\", \"PRP\", \"V\", \"ADJ\", \"NN\"]),\n",
    "    (\"i love you loads\".split(), [\"PRN\", \"V\", \"PRN\", \"ADV\"])\n",
    "]\n",
    "#  These are other words which we would like to predict (within sentences) using the model\n",
    "other_words = [\"area\", \"book\", \"business\", \"case\", \"child\", \"company\", \"country\",\n",
    "               \"day\", \"eye\", \"fact\", \"family\", \"government\", \"group\", \"hand\", \"home\",\n",
    "               \"job\", \"life\", \"lot\", \"man\", \"money\", \"month\", \"mother\", \"food\", \"night\",\n",
    "               \"number\", \"part\", \"people\", \"place\", \"point\", \"problem\", \"program\",\n",
    "               \"question\", \"right\", \"room\", \"school\", \"state\", \"story\", \"student\",\n",
    "               \"study\", \"system\", \"thing\", \"time\", \"water\", \"way\", \"week\", \"woman\",\n",
    "               \"word\", \"work\", \"world\", \"year\", \"ask\", \"be\", \"become\", \"begin\", \"can\",\n",
    "               \"come\", \"do\", \"find\", \"get\", \"go\", \"have\", \"hear\", \"keep\", \"know\", \"let\",\n",
    "               \"like\", \"look\", \"make\", \"may\", \"mean\", \"might\", \"move\", \"play\", \"put\",\n",
    "               \"run\", \"say\", \"see\", \"seem\", \"should\", \"start\", \"think\", \"try\", \"turn\",\n",
    "               \"use\", \"want\", \"will\", \"work\", \"would\", \"asked\", \"was\", \"became\", \"began\",\n",
    "               \"can\", \"come\", \"do\", \"did\", \"found\", \"got\", \"went\", \"had\", \"heard\", \"kept\",\n",
    "               \"knew\", \"let\", \"liked\", \"looked\", \"made\", \"might\", \"meant\", \"might\", \"moved\",\n",
    "               \"played\", \"put\", \"ran\", \"said\", \"saw\", \"seemed\", \"should\", \"started\",\n",
    "               \"thought\", \"tried\", \"turned\", \"used\", \"wanted\" \"worked\", \"would\", \"able\",\n",
    "               \"bad\", \"best\", \"better\", \"big\", \"black\", \"certain\", \"clear\", \"different\",\n",
    "               \"early\", \"easy\", \"economic\", \"federal\", \"free\", \"full\", \"good\", \"great\",\n",
    "               \"hard\", \"high\", \"human\", \"important\", \"international\", \"large\", \"late\",\n",
    "               \"little\", \"local\", \"long\", \"low\", \"major\", \"military\", \"national\", \"new\",\n",
    "               \"old\", \"only\", \"other\", \"political\", \"possible\", \"public\", \"real\", \"recent\",\n",
    "               \"right\", \"small\", \"social\", \"special\", \"strong\", \"sure\", \"true\", \"white\",\n",
    "               \"whole\", \"young\", \"he\", \"she\", \"it\", \"they\", \"i\", \"my\", \"mine\", \"your\", \"his\",\n",
    "               \"her\", \"father\", \"mother\", \"dog\", \"cat\", \"cow\", \"tiger\", \"a\", \"about\", \"all\",\n",
    "               \"also\", \"and\", \"as\", \"at\", \"be\", \"because\", \"but\", \"by\", \"can\", \"come\", \"could\",\n",
    "               \"day\", \"do\", \"even\", \"find\", \"first\", \"for\", \"from\", \"get\", \"give\", \"go\",\n",
    "               \"have\", \"he\", \"her\", \"here\", \"him\", \"his\", \"how\", \"I\", \"if\", \"in\", \"into\",\n",
    "               \"it\", \"its\", \"just\", \"know\", \"like\", \"look\", \"make\", \"man\", \"many\", \"me\",\n",
    "               \"more\", \"my\", \"new\", \"no\", \"not\", \"now\", \"of\", \"on\", \"one\", \"only\", \"or\",\n",
    "               \"other\", \"our\", \"out\", \"people\", \"say\", \"see\", \"she\", \"so\", \"some\", \"take\",\n",
    "               \"tell\", \"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \"these\",\n",
    "               \"they\", \"thing\", \"think\", \"this\", \"those\", \"time\", \"to\", \"two\", \"up\", \"use\",\n",
    "               \"very\", \"want\", \"way\", \"we\", \"well\", \"what\", \"when\", \"which\", \"who\", \"will\",\n",
    "               \"with\", \"would\", \"year\", \"you\", \"your\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix.keys():\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "for word in other_words:\n",
    "    if word not in word_to_ix.keys():\n",
    "        word_to_ix[word] = len(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2, \"ADJ\": 3, \"ADV\": 4, \"PRP\": 5, \"PRN\": 6}\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 64"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_score = F.log_softmax(tag_space, dim = 1)\n",
    "        return tag_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a sample tenset \n",
      " Sentence:\n",
      " everybody read the book and ate the food \n",
      " she like my dog\n",
      "[('everybody', 'ADJ'), ('read', 'ADJ'), ('the', 'ADJ'), ('book', 'ADJ'), ('and', 'ADJ'), ('ate', 'ADJ'), ('the', 'ADJ'), ('food', 'PRP')]\n",
      "[('she', 'ADJ'), ('like', 'ADJ'), ('my', 'PRP'), ('dog', 'ADJ')]\n"
     ]
    }
   ],
   "source": [
    "# test a sentence\n",
    "seq1 = \"everybody read the book and ate the food\".split()\n",
    "seq2 = \"she like my dog\".split()\n",
    "print(\"Running a sample tenset \\n Sentence:\\n {} \\n {}\".format(\" \".join(seq1),\n",
    "                                                               \" \".join(seq2)))\n",
    "with torch.no_grad():\n",
    "    for seq in [seq1, seq2]:\n",
    "        inputs = prepare_sequence(seq, word_to_ix)\n",
    "        tag_score = model(inputs)\n",
    "        max_indices = tag_score.max(dim=1)[1]\n",
    "        ret = []\n",
    "        # reverse tag_to_ix\n",
    "        reverse_tag_index = {v: k for k, v in tag_to_ix.items()}\n",
    "        for i in range(len(max_indices)):\n",
    "            idx = int(max_indices[i])\n",
    "            ret.append((seq[i], reverse_tag_index[idx]))\n",
    "        print(ret)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss 1.9361555576324463\n",
      "Epoch: 1, Loss 1.891072154045105\n",
      "Epoch: 2, Loss 1.8486526012420654\n",
      "Epoch: 3, Loss 1.8079586029052734\n",
      "Epoch: 4, Loss 1.7680761814117432\n",
      "Epoch: 5, Loss 1.7283666133880615\n",
      "Epoch: 6, Loss 1.688429355621338\n",
      "Epoch: 7, Loss 1.6480318307876587\n",
      "Epoch: 8, Loss 1.6070561408996582\n",
      "Epoch: 9, Loss 1.5654633045196533\n",
      "Epoch: 10, Loss 1.5232644081115723\n",
      "Epoch: 11, Loss 1.4805009365081787\n",
      "Epoch: 12, Loss 1.4372317790985107\n",
      "Epoch: 13, Loss 1.393524408340454\n",
      "Epoch: 14, Loss 1.349454402923584\n",
      "Epoch: 15, Loss 1.305105447769165\n",
      "Epoch: 16, Loss 1.2605704069137573\n",
      "Epoch: 17, Loss 1.215951681137085\n",
      "Epoch: 18, Loss 1.1713601350784302\n",
      "Epoch: 19, Loss 1.1269131898880005\n",
      "Epoch: 20, Loss 1.0827335119247437\n",
      "Epoch: 21, Loss 1.0389459133148193\n",
      "Epoch: 22, Loss 0.9956763982772827\n",
      "Epoch: 23, Loss 0.9530502557754517\n",
      "Epoch: 24, Loss 0.9111890196800232\n",
      "Epoch: 25, Loss 0.8702093958854675\n",
      "Epoch: 26, Loss 0.8302193880081177\n",
      "Epoch: 27, Loss 0.7913168668746948\n",
      "Epoch: 28, Loss 0.7535870671272278\n",
      "Epoch: 29, Loss 0.717101514339447\n",
      "Epoch: 30, Loss 0.6819175481796265\n",
      "Epoch: 31, Loss 0.6480785012245178\n",
      "Epoch: 32, Loss 0.6156135201454163\n",
      "Epoch: 33, Loss 0.5845394134521484\n",
      "Epoch: 34, Loss 0.5548610091209412\n",
      "Epoch: 35, Loss 0.5265727639198303\n",
      "Epoch: 36, Loss 0.49965983629226685\n",
      "Epoch: 37, Loss 0.4740990102291107\n",
      "Epoch: 38, Loss 0.449860155582428\n",
      "Epoch: 39, Loss 0.42690736055374146\n",
      "Epoch: 40, Loss 0.40519967675209045\n",
      "Epoch: 41, Loss 0.38469213247299194\n",
      "Epoch: 42, Loss 0.3653371334075928\n",
      "Epoch: 43, Loss 0.3470849096775055\n",
      "Epoch: 44, Loss 0.3298843502998352\n",
      "Epoch: 45, Loss 0.31368404626846313\n",
      "Epoch: 46, Loss 0.2984325587749481\n",
      "Epoch: 47, Loss 0.2840791642665863\n",
      "Epoch: 48, Loss 0.27057400345802307\n",
      "Epoch: 49, Loss 0.25786882638931274\n",
      "Epoch: 50, Loss 0.2459169179201126\n",
      "Epoch: 51, Loss 0.23467345535755157\n",
      "Epoch: 52, Loss 0.2240954339504242\n",
      "Epoch: 53, Loss 0.21414200961589813\n",
      "Epoch: 54, Loss 0.20477429032325745\n",
      "Epoch: 55, Loss 0.19595545530319214\n",
      "Epoch: 56, Loss 0.18765069544315338\n",
      "Epoch: 57, Loss 0.17982719838619232\n",
      "Epoch: 58, Loss 0.1724540740251541\n",
      "Epoch: 59, Loss 0.1655023843050003\n",
      "Epoch: 60, Loss 0.15894481539726257\n",
      "Epoch: 61, Loss 0.1527559459209442\n",
      "Epoch: 62, Loss 0.14691181480884552\n",
      "Epoch: 63, Loss 0.1413901448249817\n",
      "Epoch: 64, Loss 0.1361701488494873\n",
      "Epoch: 65, Loss 0.13123230636119843\n",
      "Epoch: 66, Loss 0.12655848264694214\n",
      "Epoch: 67, Loss 0.12213176488876343\n",
      "Epoch: 68, Loss 0.1179363802075386\n",
      "Epoch: 69, Loss 0.11395765095949173\n",
      "Epoch: 70, Loss 0.11018189787864685\n",
      "Epoch: 71, Loss 0.1065964326262474\n",
      "Epoch: 72, Loss 0.10318940132856369\n",
      "Epoch: 73, Loss 0.09994973242282867\n",
      "Epoch: 74, Loss 0.09686718136072159\n",
      "Epoch: 75, Loss 0.09393219649791718\n",
      "Epoch: 76, Loss 0.0911359041929245\n",
      "Epoch: 77, Loss 0.08846995234489441\n",
      "Epoch: 78, Loss 0.08592665940523148\n",
      "Epoch: 79, Loss 0.08349879086017609\n",
      "Epoch: 80, Loss 0.0811796709895134\n",
      "Epoch: 81, Loss 0.07896307110786438\n",
      "Epoch: 82, Loss 0.07684306055307388\n",
      "Epoch: 83, Loss 0.07481427490711212\n",
      "Epoch: 84, Loss 0.0728716254234314\n",
      "Epoch: 85, Loss 0.0710102841258049\n",
      "Epoch: 86, Loss 0.06922581791877747\n",
      "Epoch: 87, Loss 0.06751416623592377\n",
      "Epoch: 88, Loss 0.06587135791778564\n",
      "Epoch: 89, Loss 0.0642937496304512\n",
      "Epoch: 90, Loss 0.06277795881032944\n",
      "Epoch: 91, Loss 0.061320796608924866\n",
      "Epoch: 92, Loss 0.05991918966174126\n",
      "Epoch: 93, Loss 0.05857039615511894\n",
      "Epoch: 94, Loss 0.057271771132946014\n",
      "Epoch: 95, Loss 0.05602079629898071\n",
      "Epoch: 96, Loss 0.05481516942381859\n",
      "Epoch: 97, Loss 0.053652651607990265\n",
      "Epoch: 98, Loss 0.052531179040670395\n",
      "Epoch: 99, Loss 0.05144887417554855\n",
      "Epoch: 100, Loss 0.0504038967192173\n",
      "Epoch: 101, Loss 0.049394391477108\n",
      "Epoch: 102, Loss 0.04841887950897217\n",
      "Epoch: 103, Loss 0.04747571051120758\n",
      "Epoch: 104, Loss 0.0465635247528553\n",
      "Epoch: 105, Loss 0.04568088427186012\n",
      "Epoch: 106, Loss 0.04482652619481087\n",
      "Epoch: 107, Loss 0.04399921000003815\n",
      "Epoch: 108, Loss 0.043197739869356155\n",
      "Epoch: 109, Loss 0.04242107272148132\n",
      "Epoch: 110, Loss 0.0416681244969368\n",
      "Epoch: 111, Loss 0.04093789681792259\n",
      "Epoch: 112, Loss 0.04022952541708946\n",
      "Epoch: 113, Loss 0.03954203799366951\n",
      "Epoch: 114, Loss 0.038874633610248566\n",
      "Epoch: 115, Loss 0.03822649270296097\n",
      "Epoch: 116, Loss 0.03759690001606941\n",
      "Epoch: 117, Loss 0.036985039710998535\n",
      "Epoch: 118, Loss 0.036390312016010284\n",
      "Epoch: 119, Loss 0.03581198677420616\n",
      "Epoch: 120, Loss 0.0352494977414608\n",
      "Epoch: 121, Loss 0.03470226004719734\n",
      "Epoch: 122, Loss 0.03416959568858147\n",
      "Epoch: 123, Loss 0.03365110978484154\n",
      "Epoch: 124, Loss 0.03314617648720741\n",
      "Epoch: 125, Loss 0.03265436366200447\n",
      "Epoch: 126, Loss 0.03217519447207451\n",
      "Epoch: 127, Loss 0.03170821815729141\n",
      "Epoch: 128, Loss 0.03125298023223877\n",
      "Epoch: 129, Loss 0.030809074640274048\n",
      "Epoch: 130, Loss 0.030376141890883446\n",
      "Epoch: 131, Loss 0.02995378151535988\n",
      "Epoch: 132, Loss 0.0295416209846735\n",
      "Epoch: 133, Loss 0.029139380902051926\n",
      "Epoch: 134, Loss 0.028746675699949265\n",
      "Epoch: 135, Loss 0.02836318500339985\n",
      "Epoch: 136, Loss 0.027988629415631294\n",
      "Epoch: 137, Loss 0.027622751891613007\n",
      "Epoch: 138, Loss 0.027265209704637527\n",
      "Epoch: 139, Loss 0.026915762573480606\n",
      "Epoch: 140, Loss 0.02657419443130493\n",
      "Epoch: 141, Loss 0.02624017745256424\n",
      "Epoch: 142, Loss 0.025913549587130547\n",
      "Epoch: 143, Loss 0.02559405192732811\n",
      "Epoch: 144, Loss 0.025281481444835663\n",
      "Epoch: 145, Loss 0.024975577369332314\n",
      "Epoch: 146, Loss 0.02467622235417366\n",
      "Epoch: 147, Loss 0.02438315749168396\n",
      "Epoch: 148, Loss 0.024096235632896423\n",
      "Epoch: 149, Loss 0.023815257474780083\n",
      "Epoch: 150, Loss 0.023540044203400612\n",
      "Epoch: 151, Loss 0.02327042818069458\n",
      "Epoch: 152, Loss 0.023006288334727287\n",
      "Epoch: 153, Loss 0.022747419774532318\n",
      "Epoch: 154, Loss 0.022493723779916763\n",
      "Epoch: 155, Loss 0.022245006635785103\n",
      "Epoch: 156, Loss 0.022001171484589577\n",
      "Epoch: 157, Loss 0.02176203764975071\n",
      "Epoch: 158, Loss 0.021527545526623726\n",
      "Epoch: 159, Loss 0.02129751816391945\n",
      "Epoch: 160, Loss 0.02107182890176773\n",
      "Epoch: 161, Loss 0.020850365981459618\n",
      "Epoch: 162, Loss 0.02063305489718914\n",
      "Epoch: 163, Loss 0.020419776439666748\n",
      "Epoch: 164, Loss 0.02021039091050625\n",
      "Epoch: 165, Loss 0.020004818215966225\n",
      "Epoch: 166, Loss 0.019802970811724663\n",
      "Epoch: 167, Loss 0.019604749977588654\n",
      "Epoch: 168, Loss 0.01941007375717163\n",
      "Epoch: 169, Loss 0.019218772649765015\n",
      "Epoch: 170, Loss 0.019030919298529625\n",
      "Epoch: 171, Loss 0.01884629763662815\n",
      "Epoch: 172, Loss 0.01866484247148037\n",
      "Epoch: 173, Loss 0.01848652958869934\n",
      "Epoch: 174, Loss 0.018311230465769768\n",
      "Epoch: 175, Loss 0.01813889853656292\n",
      "Epoch: 176, Loss 0.017969485372304916\n",
      "Epoch: 177, Loss 0.017802897840738297\n",
      "Epoch: 178, Loss 0.017639029771089554\n",
      "Epoch: 179, Loss 0.01747787743806839\n",
      "Epoch: 180, Loss 0.017319409176707268\n",
      "Epoch: 181, Loss 0.01716342382133007\n",
      "Epoch: 182, Loss 0.017010025680065155\n",
      "Epoch: 183, Loss 0.016859011724591255\n",
      "Epoch: 184, Loss 0.016710445284843445\n",
      "Epoch: 185, Loss 0.016564209014177322\n",
      "Epoch: 186, Loss 0.016420278698205948\n",
      "Epoch: 187, Loss 0.016278572380542755\n",
      "Epoch: 188, Loss 0.01613905280828476\n",
      "Epoch: 189, Loss 0.01600169576704502\n",
      "Epoch: 190, Loss 0.015866434201598167\n",
      "Epoch: 191, Loss 0.01573321223258972\n",
      "Epoch: 192, Loss 0.015602009370923042\n",
      "Epoch: 193, Loss 0.01547277718782425\n",
      "Epoch: 194, Loss 0.015345454216003418\n",
      "Epoch: 195, Loss 0.01522002276033163\n",
      "Epoch: 196, Loss 0.01509641483426094\n",
      "Epoch: 197, Loss 0.014974629506468773\n",
      "Epoch: 198, Loss 0.014854589477181435\n",
      "Epoch: 199, Loss 0.014736296609044075\n",
      "Epoch: 200, Loss 0.014619702473282814\n",
      "Epoch: 201, Loss 0.014504769816994667\n",
      "Epoch: 202, Loss 0.014391457661986351\n",
      "Epoch: 203, Loss 0.014279743656516075\n",
      "Epoch: 204, Loss 0.014169576577842236\n",
      "Epoch: 205, Loss 0.014060959219932556\n",
      "Epoch: 206, Loss 0.013953845016658306\n",
      "Epoch: 207, Loss 0.013848170638084412\n",
      "Epoch: 208, Loss 0.013743970543146133\n",
      "Epoch: 209, Loss 0.01364118792116642\n",
      "Epoch: 210, Loss 0.01353977620601654\n",
      "Epoch: 211, Loss 0.013439732603728771\n",
      "Epoch: 212, Loss 0.013341017067432404\n",
      "Epoch: 213, Loss 0.013243616558611393\n",
      "Epoch: 214, Loss 0.013147498480975628\n",
      "Epoch: 215, Loss 0.013052665628492832\n",
      "Epoch: 216, Loss 0.012959024868905544\n",
      "Epoch: 217, Loss 0.012866605073213577\n",
      "Epoch: 218, Loss 0.01277536153793335\n",
      "Epoch: 219, Loss 0.012685326859354973\n",
      "Epoch: 220, Loss 0.012596418149769306\n",
      "Epoch: 221, Loss 0.012508689425885677\n",
      "Epoch: 222, Loss 0.01242201030254364\n",
      "Epoch: 223, Loss 0.012336436659097672\n",
      "Epoch: 224, Loss 0.012251905165612698\n",
      "Epoch: 225, Loss 0.012168432585895061\n",
      "Epoch: 226, Loss 0.012086034752428532\n",
      "Epoch: 227, Loss 0.012004585936665535\n",
      "Epoch: 228, Loss 0.011924147605895996\n",
      "Epoch: 229, Loss 0.011844711378216743\n",
      "Epoch: 230, Loss 0.011766217648983002\n",
      "Epoch: 231, Loss 0.011688686907291412\n",
      "Epoch: 232, Loss 0.011612052097916603\n",
      "Epoch: 233, Loss 0.011536325328052044\n",
      "Epoch: 234, Loss 0.01146149355918169\n",
      "Epoch: 235, Loss 0.01138758659362793\n",
      "Epoch: 236, Loss 0.011314532719552517\n",
      "Epoch: 237, Loss 0.01124228909611702\n",
      "Epoch: 238, Loss 0.011170887388288975\n",
      "Epoch: 239, Loss 0.011100342497229576\n",
      "Epoch: 240, Loss 0.011030599474906921\n",
      "Epoch: 241, Loss 0.010961649939417839\n",
      "Epoch: 242, Loss 0.010893484577536583\n",
      "Epoch: 243, Loss 0.010826092213392258\n",
      "Epoch: 244, Loss 0.010759436525404453\n",
      "Epoch: 245, Loss 0.010693540796637535\n",
      "Epoch: 246, Loss 0.010628369636833668\n",
      "Epoch: 247, Loss 0.01056391280144453\n",
      "Epoch: 248, Loss 0.010500204749405384\n",
      "Epoch: 249, Loss 0.010437182150781155\n",
      "Epoch: 250, Loss 0.010374832898378372\n",
      "Epoch: 251, Loss 0.010313180275261402\n",
      "Epoch: 252, Loss 0.010252194479107857\n",
      "Epoch: 253, Loss 0.010191816836595535\n",
      "Epoch: 254, Loss 0.01013214886188507\n",
      "Epoch: 255, Loss 0.010073116049170494\n",
      "Epoch: 256, Loss 0.010014638304710388\n",
      "Epoch: 257, Loss 0.009956846944987774\n",
      "Epoch: 258, Loss 0.009899642318487167\n",
      "Epoch: 259, Loss 0.009843047708272934\n",
      "Epoch: 260, Loss 0.009787020273506641\n",
      "Epoch: 261, Loss 0.009731580503284931\n",
      "Epoch: 262, Loss 0.009676706045866013\n",
      "Epoch: 263, Loss 0.009622417390346527\n",
      "Epoch: 264, Loss 0.009568694047629833\n",
      "Epoch: 265, Loss 0.009515497833490372\n",
      "Epoch: 266, Loss 0.00946282409131527\n",
      "Epoch: 267, Loss 0.009410688653588295\n",
      "Epoch: 268, Loss 0.009359137155115604\n",
      "Epoch: 269, Loss 0.009308014996349812\n",
      "Epoch: 270, Loss 0.009257476776838303\n",
      "Epoch: 271, Loss 0.009207380004227161\n",
      "Epoch: 272, Loss 0.00915780570358038\n",
      "Epoch: 273, Loss 0.00910871010273695\n",
      "Epoch: 274, Loss 0.00906010065227747\n",
      "Epoch: 275, Loss 0.009011968038976192\n",
      "Epoch: 276, Loss 0.008964297361671925\n",
      "Epoch: 277, Loss 0.008917099796235561\n",
      "Epoch: 278, Loss 0.008870330639183521\n",
      "Epoch: 279, Loss 0.008824029937386513\n",
      "Epoch: 280, Loss 0.008778192102909088\n",
      "Epoch: 281, Loss 0.008732721209526062\n",
      "Epoch: 282, Loss 0.008687741123139858\n",
      "Epoch: 283, Loss 0.00864315964281559\n",
      "Epoch: 284, Loss 0.008599018678069115\n",
      "Epoch: 285, Loss 0.008555267937481403\n",
      "Epoch: 286, Loss 0.008511911146342754\n",
      "Epoch: 287, Loss 0.008468983694911003\n",
      "Epoch: 288, Loss 0.00842646136879921\n",
      "Epoch: 289, Loss 0.008384313434362411\n",
      "Epoch: 290, Loss 0.008342545479536057\n",
      "Epoch: 291, Loss 0.008301149122416973\n",
      "Epoch: 292, Loss 0.008260191418230534\n",
      "Epoch: 293, Loss 0.008219532668590546\n",
      "Epoch: 294, Loss 0.008179295808076859\n",
      "Epoch: 295, Loss 0.008139364421367645\n",
      "Epoch: 296, Loss 0.008099832572042942\n",
      "Epoch: 297, Loss 0.008060610853135586\n",
      "Epoch: 298, Loss 0.00802176259458065\n",
      "Epoch: 299, Loss 0.007983274757862091\n",
      "Train Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "losses = []\n",
    "for epoch in range(300):\n",
    "    count = 0\n",
    "    sum_loss = 0\n",
    "    for sentence, tags in training_data:\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        out = model(sentence_in)\n",
    "        loss = loss_function(out, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        count += 1\n",
    "        sum_loss += loss\n",
    "        losses.append(sum_loss / count)\n",
    "    print(\"Epoch: {}, Loss {}\".format(epoch, losses[-1]))\n",
    "print(\"Train Finished\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# predict function\n",
    "def predict_seq(seq_list, model):\n",
    "    \"\"\"\n",
    "\n",
    "    :param seq_list: list of sequences\n",
    "    :param model: NN model\n",
    "    :return: tuple predictions\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for seq in seq_list:\n",
    "            inputs = prepare_sequence(seq, word_to_ix)\n",
    "            tags_score = model(inputs)\n",
    "            max_indices = tags_score.max(dim=1)[1]\n",
    "            pred = []\n",
    "            reverse_tag_index = {v: k for k, v in tag_to_ix.items()}\n",
    "            for i in range(len(max_indices)):\n",
    "                idx = int(max_indices[i])\n",
    "                pred.append(reverse_tag_index[idx])\n",
    "            print(\"Sequence: {} \\n\"\n",
    "              \"Tag Prediction: {}\\n\".format(seq, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: ['everybody', 'read', 'the', 'book', 'and', 'ate', 'the', 'food'] \n",
      "Tag Prediction: ['NN', 'V', 'DET', 'NN', 'NN', 'V', 'DET', 'NN']\n",
      "\n",
      "Sequence: ['she', 'like', 'my', 'dog'] \n",
      "Tag Prediction: ['NN', 'NN', 'V', 'NN']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test on unkown data\n",
    "predict_seq([seq1, seq2], model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Read in\n",
    "[back to top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def split_text(text_file, by_line=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param by_line: bool, whether to split by lines; if False, split by word\n",
    "    :param text_file: training file\n",
    "    :return: DIC, TOKENS and TAGS\n",
    "\n",
    "    \"\"\"\n",
    "    if by_line == False:\n",
    "        with open(text_file, mode=\"r\") as file:\n",
    "            text_f = file.read()\n",
    "            text_f_lst = text_f.split()\n",
    "            file.close()\n",
    "        keys, values = text_f_lst[::2], text_f_lst[1::2]\n",
    "        result_dic = dict(zip(keys, values))\n",
    "        return result_dic, keys, values\n",
    "    else:\n",
    "        with open(text_file, mode=\"r\") as file:\n",
    "            text_f = file.read()\n",
    "            text_f_lst = text_f.splitlines()\n",
    "            file.close()\n",
    "        keys = [line.split()[::2] for line in text_f_lst]\n",
    "        values = [line.split()[1::2] for line in text_f_lst]\n",
    "        # result_dic = dict(zip(keys, values))\n",
    "        return keys, values\n",
    "# create a list of list of tuples for training data\n",
    "def combine_lists(vocab_list, tags_list):\n",
    "    \"\"\"\n",
    "\n",
    "    :param vocab_list: list of sentence\n",
    "    :param tags_list:\n",
    "    :return: list of list of sentence of words tuples e.g. [[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.')]]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(len(vocab_list)):\n",
    "        sentence, tags = vocab_list[i], tags_list[i]\n",
    "        zipped = zip(sentence, tags)\n",
    "        result.append(list(zipped))\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "vocab_list, tags_list = split_text(\"wsj1-18.training\", by_line=True)\n",
    "train_list = combine_lists(vocab_list, tags_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Construct dictionary\n",
    "1. A word/tag dictionary\n",
    "2. A letter/character dictionary\n",
    "3. A POS tag dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def word_to_idx(word, ix):\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": "[None, None, None, None]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i) for i in range(4)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}