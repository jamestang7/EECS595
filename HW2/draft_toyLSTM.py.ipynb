{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import unittest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "def load_embedding(filename='glove.6B.50d.txt'):\n",
    "    \"\"\"\n",
    "    Load embedding for the training and\n",
    "    :return: dataframe, words\n",
    "    \"\"\"\n",
    "    # creat column names\n",
    "    num = np.arange(51)\n",
    "    num_str = list(map(str, num))\n",
    "    list_name = list(map(lambda x: \"dim_\" + x, num_str))\n",
    "    df = pd.read_csv(\"glove.6B.50d.txt\", sep=\" \", quoting=csv.QUOTE_NONE,\n",
    "                     header=None, encoding='utf-8',\n",
    "                     names=list_name)\n",
    "    df.rename({'dim_0': 'token'}, axis=1, inplace=True)\n",
    "    words = df.token.to_list()\n",
    "    # add padding embedding\n",
    "    df.loc['<PAD>'] = np.zeros(50)\n",
    "    df.set_index('token', inplace=True)\n",
    "    df.to_pckle(\"glove.pkl\")\n",
    "    return df, words\n",
    "def word_to_embedding(target_vocab, pre_train):\n",
    "    \"\"\"\n",
    "\n",
    "    :param pre_train: pd.DataFrame pre-trained dataframe\n",
    "    :param target_vocab: list/ array of tokens need to be transformed\n",
    "    :return: transformed matrix, result dictionary for the unique tokens\n",
    "    \"\"\"\n",
    "    matrix_len = len(target_vocab)\n",
    "    weighted_matrix = np.zeros((matrix_len + 1, 50))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(target_vocab):\n",
    "        try:\n",
    "            weighted_matrix[i] = pre_train.loc[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weighted_matrix[i] = np.random.normal(size=50)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Finished {}th words\".format(i))\n",
    "    return weighted_matrix\n",
    "def create_emb_layer(weighted_matrix1, non_trainable=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param weighted_matrix1: tensor matrix\n",
    "    :param non_trainable:\n",
    "    :return: emb_layer type embedding\n",
    "    \"\"\"\n",
    "    input_shape, embedding_dim = weighted_matrix1.shape\n",
    "    emb_layer = nn.Embedding.from_pretrained(weighted_matrix1,\n",
    "                                             padding_idx=input_shape - 1)\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    return emb_layer\n",
    "def split_text(text_file):\n",
    "    \"\"\"\n",
    "\n",
    "    :param text_file: training file\n",
    "    :return: DIC, TOKENS and TAGS\n",
    "\n",
    "    \"\"\"\n",
    "    with open(text_file, mode=\"r\") as file:\n",
    "        text_f = file.read()\n",
    "        text_f_lst = text_f.split()\n",
    "        file.close()\n",
    "    keys, values = text_f_lst[::2], text_f_lst[1::2]\n",
    "    result_dic = dict(zip(keys, values))\n",
    "    return result_dic, keys, values\n",
    "def prepare_seq(seq_list, dictionary):\n",
    "    \"\"\"\n",
    "    embedding and padded a sequence, given its relating dictionary\n",
    "    :return: padded sequence in numerical numbers\n",
    "    \"\"\"\n",
    "    embedded = []\n",
    "    for batch in seq_list:\n",
    "        empty_lst = [dictionary[tag] for tag in batch]\n",
    "        embedded.append(empty_lst)\n",
    "    # convert to a list of tensor\n",
    "    embedded = [torch.tensor(seq) for seq in embedded]\n",
    "    padded = nn.utils.rnn.pad_sequence(embedded,\n",
    "                                       batch_first=True,\n",
    "                                       padding_value=dictionary['<PAD>'])\n",
    "    return padded"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nb_layers, batch_size, nb_lstm_units, embedding_layer,\n",
    "                 bidirectional=False,\n",
    "                 dropout=0,\n",
    "                 embedding_dim=50):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layer = None\n",
    "        self.result_dic, self.words_lst, self.tags_lst = split_text(\"wsj1-18.training\")\n",
    "        self.vocab = dict(zip(sorted(set(self.words_lst)),\n",
    "                              np.arange(len(set(self.words_lst)))))\n",
    "        self.tags = dict(zip(sorted(set(self.tags_lst)),\n",
    "                             np.arange(len(set(self.tags_lst)))))\n",
    "        self.vocab['<PAD>'] = len(set(self.words_lst))\n",
    "        self.tags['<PAD>'] = len(set(self.tags_lst))\n",
    "        self.padding_idx = self.vocab['<PAD>']\n",
    "        self.nb_layers = nb_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout if nb_layers > 1 else 0\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "        # don't count the pad for the tags\n",
    "        self.nb_tags = len(self.tags) - 1\n",
    "\n",
    "        # build actual NN\n",
    "        self.__build_model()\n",
    "\n",
    "    def __build_model(self):\n",
    "        # design LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            batch_first=True,\n",
    "            num_layers=self.nb_layers,\n",
    "            bidirectional=self.bidirectional,\n",
    "            dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # output layer which project back to tag space\n",
    "        self.hidden_to_tag = nn.Linear(self.nb_lstm_units * 2\n",
    "                                       if self.bidirectional\n",
    "                                       else self.nb_lstm_units\n",
    "                                       , self.nb_tags)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # init hidden layers and input sequence length\n",
    "        h0 = torch.rand(self.nb_layers, input.size(0), self.nb_lstm_units)\n",
    "        c0 = torch.rand(self.nb_layers, input.size(0), self.nb_lstm_units)\n",
    "        input_lengths = torch.all(input != self.padding_idx, dim=2)\\\n",
    "            .sum(dim=1).flatten()\n",
    "\n",
    "        # -------------------\n",
    "        # 1. embed the input\n",
    "        # Dim transformation: (batch_size, seq_len, 1) -> (batch_size, seq_len,\n",
    "        # embedding_dim)\n",
    "        input = self.dropout_layer(self.embedding_layer(input))\n",
    "        input = input.squeeze(2)\n",
    "        # -------------------\n",
    "        # 2.  Run through LSTM\n",
    "        # Dim transformation: (B,L, embedding_dim) -> (B, L, LSTM_units)\n",
    "        input = torch.nn.utils.rnn.pack_padded_sequence(input,\n",
    "                                                        input_lengths,\n",
    "                                                        batch_first=True,\n",
    "                                                        enforce_sorted=False)\n",
    "        # now run through LSTM\n",
    "        input = input.float()\n",
    "        out, (h0, c0) = self.lstm(input, (h0, c0))  # undo the packing operation\n",
    "        out, len_unpacked = nn.utils.rnn.pad_packed_sequence(out,\n",
    "                                                             batch_first=True)\n",
    "        # -------------------\n",
    "        # 3.  Apply FC linear layer\n",
    "        # linear layer\n",
    "        out = out.view(-1,\n",
    "                       out.size(-1))  # (batch_size, seq_len, nb_lstm_units) -> (batch_size * seq_len, nb_lstm_units)\n",
    "        out = self.hidden_to_tag(out)  # (batch_size * seq_len, nb_lstm_units) -> (batch_size * seq_len, nb_tags)\n",
    "\n",
    "        # reshape into (batch_size,  seq_len, nb_lstm_units)\n",
    "        out = out.view(self.batch_size, -1, self.nb_tags)\n",
    "        # -------------------\n",
    "        # 4.  softmax to transfer it to probability\n",
    "        Y_hat = F.log_softmax(out.float(), dim=2)\n",
    "        return Y_hat\n",
    "\n",
    "    def loss(self, Y_hat, Y):\n",
    "        # NLL(tensor log_softmax output, target index list)\n",
    "        # flatten out all labels\n",
    "        Y = prepare_seq(Y, self.tags)\n",
    "        Y = Y.flatten()\n",
    "        # flatten all predictions\n",
    "        Y_hat = Y_hat.view(-1, len(self.tags) - 1)\n",
    "        # create a mask that filter '<PAD>;\n",
    "        tag_token = self.tags['<PAD>']\n",
    "        mask = (Y < tag_token)\n",
    "        mask_idx = torch.nonzero(mask.float())\n",
    "        Y_hat = Y_hat[mask_idx].squeeze(1)\n",
    "        Y = Y[mask_idx].squeeze(1)\n",
    "        loss = nn.NLLLoss()\n",
    "        result = loss(Y_hat, Y)\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "### transform text into list of words\n",
    "df = pd.read_pickle('glove.pkl')\n",
    "_, words_lst, tags_lst = split_text(\"wsj1-18.training\")\n",
    "tags = dict(zip(sorted(set(tags_lst)), np.arange(len(set(tags_lst)))))\n",
    "tags['<PAD>'] = len(tags)\n",
    "weighted_matrix = torch.load(\"weighed_matrix.pt\")\n",
    "embedding_layer_const = create_emb_layer(weighted_matrix)\n",
    "nb_layers = 2\n",
    "nb_lstm_units = 32\n",
    "batch_size = 3\n",
    "seq_len = 4\n",
    "padding_idx = 912344\n",
    "toy_training = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[172], line 22\u001B[0m\n\u001B[1;32m     16\u001B[0m model \u001B[38;5;241m=\u001B[39m LSTM(nb_layers\u001B[38;5;241m=\u001B[39mnb_layers,\n\u001B[1;32m     17\u001B[0m              batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m,\n\u001B[1;32m     18\u001B[0m              nb_lstm_units\u001B[38;5;241m=\u001B[39mnb_lstm_units,\n\u001B[1;32m     19\u001B[0m              embedding_layer\u001B[38;5;241m=\u001B[39membedding_layer_const,\n\u001B[1;32m     20\u001B[0m              bidirectional\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     21\u001B[0m iter1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(train_loader)\n\u001B[0;32m---> 22\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[28], line 49\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;66;03m# init hidden layers and input sequence length\u001B[39;00m\n\u001B[0;32m---> 49\u001B[0m     h0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnb_layers, \u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnb_lstm_units)\n\u001B[1;32m     50\u001B[0m     c0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnb_layers, \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnb_lstm_units)\n\u001B[1;32m     51\u001B[0m     input_lengths \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mall(\u001B[38;5;28minput\u001B[39m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_idx, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\\\n\u001B[1;32m     52\u001B[0m         \u001B[38;5;241m.\u001B[39msum(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mflatten()\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "batch_in = torch.tensor([[[4],\n",
    "                          [5],\n",
    "                          [912344],\n",
    "                          [912344]],\n",
    "\n",
    "                         [[6],\n",
    "                          [912344],\n",
    "                          [912344],\n",
    "                          [912344]],\n",
    "\n",
    "                         [[7],\n",
    "                          [8],\n",
    "                          [9],\n",
    "                          [10]]])\n",
    "Y = [[\"CC\", \"CD\", \"DT\"], [\"EX\"], [\"JJ\", \"IN\", \"JJ\", \"JJR\"]]\n",
    "model = LSTM(nb_layers=nb_layers,\n",
    "             batch_size=32,\n",
    "             nb_lstm_units=nb_lstm_units,\n",
    "             embedding_layer=embedding_layer_const,\n",
    "             bidirectional=False)\n",
    "iter1 = iter(train_loader)\n",
    "out = model(features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 4, 45])"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[32],\n         [32],\n         [32],\n         [20]],\n\n        [[13],\n         [14],\n         [14],\n         [20]],\n\n        [[13],\n         [14],\n         [14],\n         [14]]])"
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.max(dim=2, keepdim=True)[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "out = model(batch_in)\n",
    "# out = out.max(dim=2, keepdim=True)[1]\n",
    "Y = [[\"CC\", \"CD\", \"DT\"], [\"EX\"], [\"JJ\", \"IN\", \"JJ\", \"JJR\"]]\n",
    "# NLL(tensor log_softmax output, target index list)\n",
    "# flatten out all labels\n",
    "Y = prepare_seq(Y, tags)\n",
    "Y = Y.flatten()\n",
    "# flatten all predictions\n",
    "out = out.view(-1, len(tags) - 1)\n",
    "# create a mask that filter '<PAD>;\n",
    "tag_token = tags['<PAD>']\n",
    "mask = (Y < tag_token)\n",
    "mask_idx = torch.nonzero(mask.float())\n",
    "out = out[mask_idx].squeeze(1)\n",
    "Y = Y[mask_idx].squeeze(1)\n",
    "loss = nn.NLLLoss()\n",
    "result = loss(out, Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class SquareDataset(Dataset):\n",
    "     def __init__(self, a=0, b=1):\n",
    "         super(Dataset, self).__init__()\n",
    "         assert a <= b\n",
    "         self.a = a\n",
    "         self.b = b\n",
    "\n",
    "     def __len__(self):\n",
    "         return self.b - self.a + 1\n",
    "\n",
    "     def __getitem__(self, index):\n",
    "        assert self.a <= index <= self.b\n",
    "        return index, index**2\n",
    "\n",
    "data_train = SquareDataset(a=1,b=64)\n",
    "data_train_loader = DataLoader(data_train, batch_size=64, shuffle=True)\n",
    "print(len(data_train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TagDataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        \"\"\"\n",
    "\n",
    "        :param train: bool, if True read training data, else read testing data\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            with open(\"wsj1-18.training\", mode=\"r\") as file:\n",
    "                self.text_f = file.read()\n",
    "                self.text_f_lst = self.text_f.splitlines()\n",
    "                file.close()\n",
    "        else:\n",
    "            with open(\"wsj19-21.truth\", mode=\"r\") as file:\n",
    "                self.text_f = file.read()\n",
    "                self.text_f_lst = self.text_f.splitlines()\n",
    "                file.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_f_lst)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.text_f_lst[item].split()[0::2], self.text_f_lst[item].split()[1::2]\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[221], line 19\u001B[0m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[1;32m     16\u001B[0m toy_training \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe dog ate the apple\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39msplit(),\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEverybody read that book\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39msplit()]\n\u001B[0;32m---> 19\u001B[0m \u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoy_training\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[221], line 12\u001B[0m, in \u001B[0;36mcollate_fn\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m     10\u001B[0m result \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mones((batch_size, max_length)) \u001B[38;5;241m*\u001B[39m pad_token\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# populate the result\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, length \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(length_list):\n\u001B[1;32m     13\u001B[0m     sequence \u001B[38;5;241m=\u001B[39m list_sentence[i]\n\u001B[1;32m     14\u001B[0m     result[i][\u001B[38;5;241m0\u001B[39m:length] \u001B[38;5;241m=\u001B[39m sequence\n",
      "Cell \u001B[0;32mIn[221], line 12\u001B[0m, in \u001B[0;36mcollate_fn\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m     10\u001B[0m result \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mones((batch_size, max_length)) \u001B[38;5;241m*\u001B[39m pad_token\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# populate the result\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, length \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(length_list):\n\u001B[1;32m     13\u001B[0m     sequence \u001B[38;5;241m=\u001B[39m list_sentence[i]\n\u001B[1;32m     14\u001B[0m     result[i][\u001B[38;5;241m0\u001B[39m:length] \u001B[38;5;241m=\u001B[39m sequence\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:747\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    745\u001B[0m \u001B[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001B[39;00m\n\u001B[1;32m    746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info\u001B[38;5;241m.\u001B[39mpydev_state \u001B[38;5;241m==\u001B[39m STATE_SUSPEND:\n\u001B[0;32m--> 747\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;66;03m# No need to reset frame.f_trace to keep the same trace function.\u001B[39;00m\n\u001B[1;32m    749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrace_dispatch\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:144\u001B[0m, in \u001B[0;36mPyDBFrame.do_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 144\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1147\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1144\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1147\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1162\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1159\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1161\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1162\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1166\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "vocab = dict(zip(sorted(set(words_lst)), np.arange(len(set(words_lst)))))\n",
    "vocab['<PAD>'] = 912344\n",
    "def collate_fn(batch):\n",
    "    list_sentence = [[vocab[word] for word in sentence] for sentence in batch]\n",
    "    length_list = [len(sentence) for sentence in list_sentence]\n",
    "    max_length = max(length_list)\n",
    "    batch_size = len(list_sentence)\n",
    "    pad_token = vocab['<PAD>']\n",
    "    # init tensors of ones with batch_size * max_length\n",
    "    result = np.ones((batch_size, max_length)) * pad_token\n",
    "    # populate the result\n",
    "    for i, length in enumerate(length_list):\n",
    "        sequence = list_sentence[i]\n",
    "        result[i][0:length] = sequence\n",
    "    return torch.from_numpy(result)\n",
    "toy_training = [\n",
    "    \"The dog ate the apple\".split(),\n",
    "    \"Everybody read that book\".split()]\n",
    "collate_fn(toy_training)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "\n",
    "# todo: prepare dataloader for text\n",
    "train_loader = DataLoader(TagDataset(train=True),\n",
    "                          batch_size=batch_size,\n",
    "                          collate_fn=collate_fn,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(TagDataset(train=False),\n",
    "                         batch_size=batch_size,\n",
    "                         collate_fn=collate_fn,\n",
    "                         shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [],
   "source": [
    "iter1 = iter(train_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[192], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m x, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miter1\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:61\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[190], line 29\u001B[0m, in \u001B[0;36mcollate_fn\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcollate_fn\u001B[39m(batch):\n\u001B[1;32m     28\u001B[0m     list_tensor \u001B[38;5;241m=\u001B[39m [i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m batch]\n\u001B[0;32m---> 29\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpad_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlist_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_first\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m<PAD>\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/py3.8/lib/python3.8/site-packages/torch/nn/utils/rnn.py:398\u001B[0m, in \u001B[0;36mpad_sequence\u001B[0;34m(sequences, batch_first, padding_value)\u001B[0m\n\u001B[1;32m    394\u001B[0m         sequences \u001B[38;5;241m=\u001B[39m sequences\u001B[38;5;241m.\u001B[39munbind(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    396\u001B[0m \u001B[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001B[39;00m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001B[39;00m\n\u001B[0;32m--> 398\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_value\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: expected Tensor as element 0 in argument 0, but got tuple"
     ]
    }
   ],
   "source": [
    "x, y = next(iter1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [],
   "source": [
    "    num = np.arange(51)\n",
    "    num_str = list(map(str, num))\n",
    "    list_name = list(map(lambda x: \"dim_\" + x, num_str))\n",
    "    df = pd.read_csv(\"glove.6B.50d.txt\", sep=\" \", quoting=csv.QUOTE_NONE,\n",
    "                     header=None, encoding='utf-8',\n",
    "                     names=list_name)\n",
    "    df.rename({'dim_0': 'token'}, axis=1, inplace=True)\n",
    "    words = df.token.to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[227], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m<PAD>\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m50\u001B[39m)\n",
      "File \u001B[0;32m~/py3.8/lib/python3.8/site-packages/pandas/core/indexing.py:818\u001B[0m, in \u001B[0;36m_LocationIndexer.__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m    815\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_valid_setitem_indexer(key)\n\u001B[1;32m    817\u001B[0m iloc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miloc\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39miloc\n\u001B[0;32m--> 818\u001B[0m \u001B[43miloc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setitem_with_indexer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/py3.8/lib/python3.8/site-packages/pandas/core/indexing.py:1785\u001B[0m, in \u001B[0;36m_iLocIndexer._setitem_with_indexer\u001B[0;34m(self, indexer, value, name)\u001B[0m\n\u001B[1;32m   1782\u001B[0m     indexer, missing \u001B[38;5;241m=\u001B[39m convert_missing_indexer(indexer)\n\u001B[1;32m   1784\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m missing:\n\u001B[0;32m-> 1785\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setitem_with_indexer_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   1788\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloc\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1789\u001B[0m     \u001B[38;5;66;03m# must come after setting of missing\u001B[39;00m\n",
      "File \u001B[0;32m~/py3.8/lib/python3.8/site-packages/pandas/core/indexing.py:2160\u001B[0m, in \u001B[0;36m_iLocIndexer._setitem_with_indexer_missing\u001B[0;34m(self, indexer, value)\u001B[0m\n\u001B[1;32m   2157\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_list_like_indexer(value):\n\u001B[1;32m   2158\u001B[0m         \u001B[38;5;66;03m# must have conforming columns\u001B[39;00m\n\u001B[1;32m   2159\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(value) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39mcolumns):\n\u001B[0;32m-> 2160\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot set a row with mismatched columns\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2162\u001B[0m     value \u001B[38;5;241m=\u001B[39m Series(value, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39mcolumns, name\u001B[38;5;241m=\u001B[39mindexer)\n\u001B[1;32m   2164\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj):\n\u001B[1;32m   2165\u001B[0m     \u001B[38;5;66;03m# We will ignore the existing dtypes instead of using\u001B[39;00m\n\u001B[1;32m   2166\u001B[0m     \u001B[38;5;66;03m#  internals.concat logic\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "df.loc['<PAD>'] = np.zeros(50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [],
   "source": [
    "df.set_index('token', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "data": {
      "text/plain": "               dim_1     dim_2    dim_3     dim_4    dim_5     dim_6  \\\ntoken                                                                  \nthe         0.418000  0.249680 -0.41242  0.121700  0.34527 -0.044457   \n,           0.013441  0.236820 -0.16899  0.409510  0.63812  0.477090   \n.           0.151640  0.301770 -0.16763  0.176840  0.31719  0.339730   \nof          0.708530  0.570880 -0.47160  0.180480  0.54449  0.726030   \nto          0.680470 -0.039263  0.30186 -0.177920  0.42962  0.032246   \n...              ...       ...      ...       ...      ...       ...   \nchanty      0.232040  0.025672 -0.70699 -0.045465  0.13989 -0.628070   \nkronik     -0.609210 -0.672180  0.23521 -0.111950 -0.46094 -0.007462   \nrolonda    -0.511810  0.058706  1.09130 -0.551630 -0.10249 -0.126500   \nzsombor    -0.758980 -0.474260  0.47370  0.772500 -0.78064  0.232330   \nsandberger  0.072617 -0.513930  0.47280 -0.522020 -0.35534  0.346290   \n\n               dim_7     dim_8     dim_9    dim_10  ...    dim_41    dim_42  \\\ntoken                                               ...                       \nthe        -0.496880 -0.178620 -0.000660 -0.656600  ... -0.298710 -0.157490   \n,          -0.428520 -0.556410 -0.364000 -0.239380  ... -0.080262  0.630030   \n.          -0.434780 -0.310860 -0.449990 -0.294860  ... -0.000064  0.068987   \nof          0.181570 -0.523930  0.103810 -0.175660  ... -0.347270  0.284830   \nto         -0.413760  0.132280 -0.298470 -0.085253  ... -0.094375  0.018324   \n...              ...       ...       ...       ...  ...       ...       ...   \nchanty      0.726250  0.341080  0.446140  0.163290  ... -0.095526 -0.296050   \nkronik      0.255780  0.856320  0.055977 -0.237920  ...  0.672050 -0.598220   \nrolonda     0.995030  0.079711 -0.162460  0.564880  ...  0.024747  0.200920   \nzsombor     0.046114  0.840140  0.243710  0.022978  ...  0.454390 -0.842540   \nsandberger  0.232110  0.230960  0.266940  0.410280  ...  0.688800 -0.179860   \n\n              dim_43    dim_44    dim_45    dim_46    dim_47    dim_48  \\\ntoken                                                                    \nthe        -0.347580 -0.045637 -0.442510  0.187850  0.002785 -0.184110   \n,           0.321110 -0.467650  0.227860  0.360340 -0.378180 -0.566570   \n.           0.087939 -0.102850 -0.139310  0.223140 -0.080803 -0.356520   \nof          0.075693 -0.062178 -0.389880  0.229020 -0.216170 -0.225620   \nto          0.210480 -0.030880 -0.197220  0.082279 -0.094340 -0.073297   \n...              ...       ...       ...       ...       ...       ...   \nchanty      0.385670  0.136840  0.593310 -0.694860  0.124100 -0.180690   \nkronik     -0.202590  0.392430  0.028873  0.030003 -0.106170 -0.114110   \nrolonda    -1.085100 -0.136260  0.350520 -0.858910  0.067858 -0.250030   \nzsombor     0.106500 -0.059397  0.090449  0.305810 -0.614240  0.789540   \nsandberger -0.066569 -0.480440 -0.559460 -0.275940  0.056072 -0.189070   \n\n              dim_49    dim_50  \ntoken                           \nthe        -0.115140 -0.785810  \n,           0.044691  0.303920  \n.           0.016413  0.102160  \nof         -0.093918 -0.803750  \nto         -0.064699 -0.260440  \n...              ...       ...  \nchanty     -0.258300 -0.039673  \nkronik     -0.249010 -0.120260  \nrolonda    -1.125000  1.586300  \nzsombor    -0.014116  0.644800  \nsandberger -0.590210  0.555590  \n\n[400000 rows x 50 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dim_1</th>\n      <th>dim_2</th>\n      <th>dim_3</th>\n      <th>dim_4</th>\n      <th>dim_5</th>\n      <th>dim_6</th>\n      <th>dim_7</th>\n      <th>dim_8</th>\n      <th>dim_9</th>\n      <th>dim_10</th>\n      <th>...</th>\n      <th>dim_41</th>\n      <th>dim_42</th>\n      <th>dim_43</th>\n      <th>dim_44</th>\n      <th>dim_45</th>\n      <th>dim_46</th>\n      <th>dim_47</th>\n      <th>dim_48</th>\n      <th>dim_49</th>\n      <th>dim_50</th>\n    </tr>\n    <tr>\n      <th>token</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>the</th>\n      <td>0.418000</td>\n      <td>0.249680</td>\n      <td>-0.41242</td>\n      <td>0.121700</td>\n      <td>0.34527</td>\n      <td>-0.044457</td>\n      <td>-0.496880</td>\n      <td>-0.178620</td>\n      <td>-0.000660</td>\n      <td>-0.656600</td>\n      <td>...</td>\n      <td>-0.298710</td>\n      <td>-0.157490</td>\n      <td>-0.347580</td>\n      <td>-0.045637</td>\n      <td>-0.442510</td>\n      <td>0.187850</td>\n      <td>0.002785</td>\n      <td>-0.184110</td>\n      <td>-0.115140</td>\n      <td>-0.785810</td>\n    </tr>\n    <tr>\n      <th>,</th>\n      <td>0.013441</td>\n      <td>0.236820</td>\n      <td>-0.16899</td>\n      <td>0.409510</td>\n      <td>0.63812</td>\n      <td>0.477090</td>\n      <td>-0.428520</td>\n      <td>-0.556410</td>\n      <td>-0.364000</td>\n      <td>-0.239380</td>\n      <td>...</td>\n      <td>-0.080262</td>\n      <td>0.630030</td>\n      <td>0.321110</td>\n      <td>-0.467650</td>\n      <td>0.227860</td>\n      <td>0.360340</td>\n      <td>-0.378180</td>\n      <td>-0.566570</td>\n      <td>0.044691</td>\n      <td>0.303920</td>\n    </tr>\n    <tr>\n      <th>.</th>\n      <td>0.151640</td>\n      <td>0.301770</td>\n      <td>-0.16763</td>\n      <td>0.176840</td>\n      <td>0.31719</td>\n      <td>0.339730</td>\n      <td>-0.434780</td>\n      <td>-0.310860</td>\n      <td>-0.449990</td>\n      <td>-0.294860</td>\n      <td>...</td>\n      <td>-0.000064</td>\n      <td>0.068987</td>\n      <td>0.087939</td>\n      <td>-0.102850</td>\n      <td>-0.139310</td>\n      <td>0.223140</td>\n      <td>-0.080803</td>\n      <td>-0.356520</td>\n      <td>0.016413</td>\n      <td>0.102160</td>\n    </tr>\n    <tr>\n      <th>of</th>\n      <td>0.708530</td>\n      <td>0.570880</td>\n      <td>-0.47160</td>\n      <td>0.180480</td>\n      <td>0.54449</td>\n      <td>0.726030</td>\n      <td>0.181570</td>\n      <td>-0.523930</td>\n      <td>0.103810</td>\n      <td>-0.175660</td>\n      <td>...</td>\n      <td>-0.347270</td>\n      <td>0.284830</td>\n      <td>0.075693</td>\n      <td>-0.062178</td>\n      <td>-0.389880</td>\n      <td>0.229020</td>\n      <td>-0.216170</td>\n      <td>-0.225620</td>\n      <td>-0.093918</td>\n      <td>-0.803750</td>\n    </tr>\n    <tr>\n      <th>to</th>\n      <td>0.680470</td>\n      <td>-0.039263</td>\n      <td>0.30186</td>\n      <td>-0.177920</td>\n      <td>0.42962</td>\n      <td>0.032246</td>\n      <td>-0.413760</td>\n      <td>0.132280</td>\n      <td>-0.298470</td>\n      <td>-0.085253</td>\n      <td>...</td>\n      <td>-0.094375</td>\n      <td>0.018324</td>\n      <td>0.210480</td>\n      <td>-0.030880</td>\n      <td>-0.197220</td>\n      <td>0.082279</td>\n      <td>-0.094340</td>\n      <td>-0.073297</td>\n      <td>-0.064699</td>\n      <td>-0.260440</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>chanty</th>\n      <td>0.232040</td>\n      <td>0.025672</td>\n      <td>-0.70699</td>\n      <td>-0.045465</td>\n      <td>0.13989</td>\n      <td>-0.628070</td>\n      <td>0.726250</td>\n      <td>0.341080</td>\n      <td>0.446140</td>\n      <td>0.163290</td>\n      <td>...</td>\n      <td>-0.095526</td>\n      <td>-0.296050</td>\n      <td>0.385670</td>\n      <td>0.136840</td>\n      <td>0.593310</td>\n      <td>-0.694860</td>\n      <td>0.124100</td>\n      <td>-0.180690</td>\n      <td>-0.258300</td>\n      <td>-0.039673</td>\n    </tr>\n    <tr>\n      <th>kronik</th>\n      <td>-0.609210</td>\n      <td>-0.672180</td>\n      <td>0.23521</td>\n      <td>-0.111950</td>\n      <td>-0.46094</td>\n      <td>-0.007462</td>\n      <td>0.255780</td>\n      <td>0.856320</td>\n      <td>0.055977</td>\n      <td>-0.237920</td>\n      <td>...</td>\n      <td>0.672050</td>\n      <td>-0.598220</td>\n      <td>-0.202590</td>\n      <td>0.392430</td>\n      <td>0.028873</td>\n      <td>0.030003</td>\n      <td>-0.106170</td>\n      <td>-0.114110</td>\n      <td>-0.249010</td>\n      <td>-0.120260</td>\n    </tr>\n    <tr>\n      <th>rolonda</th>\n      <td>-0.511810</td>\n      <td>0.058706</td>\n      <td>1.09130</td>\n      <td>-0.551630</td>\n      <td>-0.10249</td>\n      <td>-0.126500</td>\n      <td>0.995030</td>\n      <td>0.079711</td>\n      <td>-0.162460</td>\n      <td>0.564880</td>\n      <td>...</td>\n      <td>0.024747</td>\n      <td>0.200920</td>\n      <td>-1.085100</td>\n      <td>-0.136260</td>\n      <td>0.350520</td>\n      <td>-0.858910</td>\n      <td>0.067858</td>\n      <td>-0.250030</td>\n      <td>-1.125000</td>\n      <td>1.586300</td>\n    </tr>\n    <tr>\n      <th>zsombor</th>\n      <td>-0.758980</td>\n      <td>-0.474260</td>\n      <td>0.47370</td>\n      <td>0.772500</td>\n      <td>-0.78064</td>\n      <td>0.232330</td>\n      <td>0.046114</td>\n      <td>0.840140</td>\n      <td>0.243710</td>\n      <td>0.022978</td>\n      <td>...</td>\n      <td>0.454390</td>\n      <td>-0.842540</td>\n      <td>0.106500</td>\n      <td>-0.059397</td>\n      <td>0.090449</td>\n      <td>0.305810</td>\n      <td>-0.614240</td>\n      <td>0.789540</td>\n      <td>-0.014116</td>\n      <td>0.644800</td>\n    </tr>\n    <tr>\n      <th>sandberger</th>\n      <td>0.072617</td>\n      <td>-0.513930</td>\n      <td>0.47280</td>\n      <td>-0.522020</td>\n      <td>-0.35534</td>\n      <td>0.346290</td>\n      <td>0.232110</td>\n      <td>0.230960</td>\n      <td>0.266940</td>\n      <td>0.410280</td>\n      <td>...</td>\n      <td>0.688800</td>\n      <td>-0.179860</td>\n      <td>-0.066569</td>\n      <td>-0.480440</td>\n      <td>-0.559460</td>\n      <td>-0.275940</td>\n      <td>0.056072</td>\n      <td>-0.189070</td>\n      <td>-0.590210</td>\n      <td>0.555590</td>\n    </tr>\n  </tbody>\n</table>\n<p>400000 rows × 50 columns</p>\n</div>"
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}