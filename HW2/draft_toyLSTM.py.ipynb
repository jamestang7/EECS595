{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import unittest\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.has_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def load_embedding(filename='glove.6B.50d.txt'):\n",
    "    \"\"\"\n",
    "    Load embedding for the training and\n",
    "    :return: dataframe, words\n",
    "    \"\"\"\n",
    "    # creat column names\n",
    "    num = np.arange(51)\n",
    "    num_str = list(map(str, num))\n",
    "    list_name = list(map(lambda x: \"dim_\" + x, num_str))\n",
    "    df = pd.read_csv(\"glove.6B.50d.txt\", sep=\" \", quoting=csv.QUOTE_NONE,\n",
    "                     header=None, encoding='utf-8',\n",
    "                     names=list_name)\n",
    "    df.rename({'dim_0': 'token'}, axis=1, inplace=True)\n",
    "    words = df.token.to_list()\n",
    "    # add padding embedding\n",
    "    df.set_index('token', inplace=True)\n",
    "    df.loc['<PAD>'] = np.zeros(50)\n",
    "    df.to_pickle(\"glove.pkl\")\n",
    "    return df, words\n",
    "\n",
    "\n",
    "def word_to_embedding(target_vocab, pre_train):\n",
    "    \"\"\"\n",
    "\n",
    "    :param pre_train: pd.DataFrame pre-trained dataframe\n",
    "    :param target_vocab: list/ array of tokens need to be transformed\n",
    "    :return: transformed matrix, result dictionary for the unique tokens\n",
    "    \"\"\"\n",
    "    matrix_len = len(target_vocab)\n",
    "    weighted_matrix = np.zeros((matrix_len + 1, 50))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(target_vocab):\n",
    "        try:\n",
    "            weighted_matrix[i] = pre_train.loc[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weighted_matrix[i] = np.random.normal(size=50)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Finished {}th words\".format(i))\n",
    "    return weighted_matrix\n",
    "\n",
    "\n",
    "def create_emb_layer(weighted_matrix1, non_trainable=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param weighted_matrix1: tensor matrix\n",
    "    :param non_trainable:\n",
    "    :return: emb_layer type embedding\n",
    "    \"\"\"\n",
    "    input_shape, embedding_dim = weighted_matrix1.shape\n",
    "    if type(weighted_matrix1) == np.ndarray:\n",
    "        weighted_matrix1 = torch.from_numpy(weighted_matrix1)\n",
    "    emb_layer = nn.Embedding.from_pretrained(weighted_matrix1,\n",
    "                                             padding_idx=input_shape - 1)\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    return emb_layer\n",
    "\n",
    "\n",
    "def split_text(text_file, by_line=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param text_file: training file\n",
    "    :return: DIC, TOKENS and TAGS\n",
    "\n",
    "    \"\"\"\n",
    "    if by_line == False:\n",
    "        with open(text_file, mode=\"r\") as file:\n",
    "            text_f = file.read()\n",
    "            text_f_lst = text_f.split()\n",
    "            file.close()\n",
    "        keys, values = text_f_lst[::2], text_f_lst[1::2]\n",
    "        result_dic = dict(zip(keys, values))\n",
    "        return result_dic, keys, values\n",
    "    else:\n",
    "        with open(text_file, mode=\"r\") as file:\n",
    "            text_f = file.read()\n",
    "            text_f_lst = text_f.splitlines()\n",
    "            file.close()\n",
    "        keys = [line.split()[::2] for line in text_f_lst]\n",
    "        values = [line.split()[1::2] for line in text_f_lst]\n",
    "        # result_dic = dict(zip(keys, values))\n",
    "        return keys, values\n",
    "\n",
    "\n",
    "def prepare_seq(seq_list, dictionary):\n",
    "    \"\"\"\n",
    "    embedding and padded a sequence, given its relating dictionary\n",
    "    :return: padded sequence in numerical numbers\n",
    "    \"\"\"\n",
    "    embedded = []\n",
    "    for batch in seq_list:\n",
    "        empty_lst = [dictionary[tag] for tag in batch]\n",
    "        embedded.append(empty_lst)\n",
    "    embedded = [torch.tensor(seq) for seq in embedded]\n",
    "    padded = nn.utils.rnn.pad_sequence(embedded,\n",
    "                                       batch_first=True,\n",
    "                                       padding_value=dictionary['<PAD>'])\n",
    "    print(padded)\n",
    "    return padded\n",
    "\n",
    "\n",
    "def get_length_tensor(batch, padding_idx=912344):\n",
    "    index = batch.size(0)\n",
    "    result = []\n",
    "    for i in range(index):\n",
    "        sentence = batch[i]\n",
    "        length = len(\n",
    "            torch.nonzero(sentence != padding_idx))  # grab ith tensor's length\n",
    "        result.append(length)\n",
    "    return result\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nb_layers, batch_size, nb_lstm_units, embedding_layer,\n",
    "                 bidirectional=False,\n",
    "                 dropout=0,\n",
    "                 embedding_dim=50):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layer = None\n",
    "        self.result_dic, self.words_lst, self.tags_lst = split_text(\n",
    "            \"wsj1-18.training\")\n",
    "        self.vocab = dict(zip(sorted(set(self.words_lst)),\n",
    "                              np.arange(len(set(self.words_lst)))))\n",
    "        self.tags = dict(zip(sorted(set(self.tags_lst)),\n",
    "                             np.arange(len(set(self.tags_lst)))))\n",
    "        self.vocab['<PAD>'] = len(set(self.words_lst))\n",
    "        self.tags['<PAD>'] = len(set(self.tags_lst))\n",
    "        self.padding_idx = self.vocab['<PAD>']\n",
    "        self.nb_layers = nb_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout if nb_layers > 1 else 0\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "        # don't count the pad for the tags\n",
    "        self.nb_tags = len(self.tags) - 1\n",
    "\n",
    "        # build actual NN\n",
    "        self.__build_model()\n",
    "\n",
    "    def __build_model(self):\n",
    "        # design LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            batch_first=True,\n",
    "            num_layers=self.nb_layers,\n",
    "            bidirectional=self.bidirectional,\n",
    "            dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # output layer which project back to tag space\n",
    "        self.hidden_to_tag = nn.Linear(self.nb_lstm_units * 2\n",
    "                                       if self.bidirectional\n",
    "                                       else self.nb_lstm_units\n",
    "                                       , self.nb_tags)\n",
    "\n",
    "    def forward(self, input, *args, **kwargs):\n",
    "        # init hidden layers and input sequence length\n",
    "        input.to(device)\n",
    "        h0 = torch.rand(self.nb_layers * 2 if self.bidirectional else self.nb_layers, input.size(0), self.nb_lstm_units).to(device)\n",
    "        c0 = torch.rand(self.nb_layers * 2 if self.bidirectional else self.nb_layers, input.size(0), self.nb_lstm_units).to(device)\n",
    "        input_lengths = get_length_tensor(input)\n",
    "\n",
    "        # -------------------\n",
    "        # 1. embed the input\n",
    "        # Dim transformation: (batch_size, seq_len, 1) -> (batch_size, seq_len,\n",
    "        # embedding_dim)\n",
    "        input = self.embedding_layer(input.long())\n",
    "        input = input.squeeze(2)\n",
    "        # -------------------\n",
    "        # 2.  Run through LSTM\n",
    "        # Dim transformation: (B,L, embedding_dim) -> (B, L, LSTM_units)\n",
    "        input = torch.nn.utils.rnn.pack_padded_sequence(input,\n",
    "                                                        input_lengths,\n",
    "                                                        batch_first=True,\n",
    "                                                        enforce_sorted=False)\n",
    "        # now run through LSTM\n",
    "        input = input.float()\n",
    "        out, (h0, c0) = self.lstm(input, (h0, c0))  # undo the packing operation\n",
    "        out, len_unpacked = nn.utils.rnn.pad_packed_sequence(out,\n",
    "                                                             batch_first=True)\n",
    "        # Dropout\n",
    "        out = self.dropout_layer(out)\n",
    "        \n",
    "        # -------------------\n",
    "        # 3.  Apply FC linear layer\n",
    "        # linear layer\n",
    "        out = out.view(-1, out.size(\n",
    "            -1))  # (batch_size, seq_len, nb_lstm_units) -> (batch_size * seq_len, nb_lstm_units)\n",
    "        out = self.hidden_to_tag(\n",
    "            out)  # (batch_size * seq_len, nb_lstm_units) -> (batch_size * seq_len, nb_tags)\n",
    "\n",
    "        # reshape into (batch_size,  seq_len, nb_lstm_units)\n",
    "        out = out.view(self.batch_size, -1, self.nb_tags)\n",
    "        # -------------------\n",
    "        # 4.  softmax to transfer it to probability\n",
    "        # Y_hat = F.log_softmax(out.float(), dim=2)\n",
    "        return out\n",
    "\n",
    "    def loss(self, Y_hat, Y):\n",
    "        # NLL(tensor log_softmax output, target index list)\n",
    "        # flatten out all labels\n",
    "        ## next line deprecated because Y is already padded in data loader\n",
    "        # Y = prepare_seq(Y, self.tags)  # convert labels into number by tag dict\n",
    "        # Y = Y.flatten()\n",
    "        # # flatten all predictions\n",
    "        # Y_hat = Y_hat.view(-1, len(self.tags) - 1)\n",
    "        # # create a mask that filter '<PAD>;\n",
    "        tag_token = self.tags['<PAD>']\n",
    "        # mask = (Y < tag_token)\n",
    "        # mask_idx = torch.nonzero(mask.float())\n",
    "        # Y_hat = Y_hat[mask_idx].squeeze(1)\n",
    "        # Y = Y[mask_idx].squeeze(1)\n",
    "        # loss = nn.NLLLoss()\n",
    "        # result = loss(Y_hat, Y)\n",
    "\n",
    "        ### second approac using ignore_idx = 45\n",
    "        ### flatten Y_hat and apply log_softmax\n",
    "        Y_hat = Y_hat.view(-1, tag_token).float()\n",
    "        Y_hat = F.log_softmax(Y_hat, dim=1).double()\n",
    "        Y = Y.flatten().long()\n",
    "        loss = nn.NLLLoss(ignore_index=tag_token)\n",
    "        result = loss(Y_hat, Y)\n",
    "        return result\n",
    "\n",
    "\n",
    "class TagDataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        \"\"\"\n",
    "\n",
    "        :param train: bool, if True read training data, else read testing data\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        if self.train:\n",
    "\n",
    "            self.trainX, self.trainY = split_text(\n",
    "                \"wsj1-18.training\", by_line=True)\n",
    "        else:\n",
    "            self.testX, self.testY = split_text(\"wsj19-21.truth\",\n",
    "                                                by_line=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trainY) if self.train else len(self.testY)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.train:\n",
    "            return self.trainX[item], self.trainY[item]\n",
    "        else:\n",
    "            return self.testX[item], self.testY[item]\n",
    "\n",
    "\n",
    "#  customized collate_fn to get to equal size\n",
    "def collate_fn(batch):\n",
    "    def helper(target, batch):\n",
    "        if target == 'X':\n",
    "            dic = vocab\n",
    "            list_sentence = [item[0] for item in batch]\n",
    "        else:\n",
    "            dic = tags\n",
    "            list_sentence = [item[1] for item in batch]\n",
    "        # if test not in training dic\n",
    "        try:\n",
    "            list_sentence = [[dic[word] for word in sentence] for sentence in\n",
    "                             list_sentence]\n",
    "        except:\n",
    "            list_sentence_temp = []\n",
    "            for sentence in list_sentence:\n",
    "                _ = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        temp = dic[word]\n",
    "                    except:\n",
    "                        temp = dic['James']\n",
    "                    _.append(temp)\n",
    "                list_sentence_temp.append(_)\n",
    "                list_sentence = list_sentence_temp\n",
    "        length_list = [len(sentence) for sentence in list_sentence]\n",
    "        max_length = max(length_list)\n",
    "        batch_size = len(list_sentence)\n",
    "        pad_token = dic['<PAD>']\n",
    "        # init tensors of ones with batch_size * max_length\n",
    "        result = np.ones((batch_size, max_length)) * pad_token\n",
    "        # populate the result\n",
    "        for i, length in enumerate(length_list):\n",
    "            sequence = list_sentence[i]\n",
    "            result[i][0:length] = sequence\n",
    "        return torch.from_numpy(result).to(device)\n",
    "\n",
    "    return [helper('X', batch), helper('Y', batch)]\n",
    "\n",
    "# done: accuracy computation\n",
    "def get_accuracy(model, train):\n",
    "    data = train_loader if train else test_loader\n",
    "    correct, total = 0, 0\n",
    "    model = model.to(device)\n",
    "    for X, labels in data:\n",
    "        X, labels = X.to(device), labels.to(device)\n",
    "        tag_padding_token = tags['<PAD>']\n",
    "        y_pred = model(X)\n",
    "        y_pred = F.softmax(y_pred, dim=2)  # change into probability\n",
    "        # select the maximum probablilty in dim2, out of all tags\n",
    "        y_pred = y_pred.max(dim=2)[1]\n",
    "        # flatten all prediction\n",
    "        y_pred, labels = y_pred.flatten(), labels.flatten()\n",
    "        mask = (labels < tag_padding_token)\n",
    "        y_pred, labels = y_pred[mask], labels[mask]\n",
    "        correct += labels.eq(y_pred).sum().item()\n",
    "        total += len(y_pred)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def train(model, num_epoch=1):\n",
    "    iters, losses, train_acc, test_acc = [], [], [], []\n",
    "    model = model.to(device=device)\n",
    "    # training\n",
    "    n = 0  # number of iterations\n",
    "    for epoch in range(num_epoch):\n",
    "        start = time.time()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # print(\"Batch:{}\".format(batch_idx+1))\n",
    "            out = model(data)  # forward pass\n",
    "            loss = model.loss(out, target)  # compute the loss\n",
    "            loss.backward()  # backwardpass (compute parameter updates)\n",
    "            optimizer.step()  # make the update to each parameter\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # save the current training log\n",
    "            iters.append(n)\n",
    "            losses.append(float(loss) / batch_size)\n",
    "\n",
    "            n += 1\n",
    "            # print result\n",
    "            if batch_idx % 500 == 0:            \n",
    "                train_acc.append(get_accuracy(model, train=True))\n",
    "                test_acc.append(get_accuracy(model, train=False))\n",
    "                print(f\"Epoch: {epoch + 1}; Batch: {batch_idx + 1}; \"\n",
    "                      f\"Loss: {float(loss) / batch_size};\"\n",
    "                      f\"Training Acc:{train_acc[-1]:.2%};\"\n",
    "                      f\"Testing Acc:{test_acc[-1]:.2%}\")\n",
    "        end = time.time()\n",
    "        print(f\"Epoch training time {end - start}\")\n",
    "\n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(iters, losses, label=\"Train\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training/Testing Curve with Accuracy\")\n",
    "    plt.plot(np.arange(len(train_acc)), train_acc, label=\"Train\")\n",
    "    plt.plot(np.arange(len(test_acc)), test_acc, label=\"Test\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Final training Accuracy: {:.2%}\".format(train_acc[-1]))\n",
    "    print(\"Final testing Accuracy: {:.2%}\".format(test_acc[-1]))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Batch: 1; Loss: 0.029771748242195126;Training Acc:1.06%;Testing Acc:1.14%\n",
      "Epoch training time 53.33979558944702\n",
      "Epoch: 2; Batch: 1; Loss: 0.021353089298274;Training Acc:18.60%;Testing Acc:18.86%\n",
      "Epoch training time 53.12846350669861\n",
      "Epoch: 3; Batch: 1; Loss: 0.02110246840369193;Training Acc:19.43%;Testing Acc:19.56%\n",
      "Epoch training time 52.331867933273315\n",
      "Epoch: 4; Batch: 1; Loss: 0.020804804779092793;Training Acc:21.90%;Testing Acc:22.14%\n",
      "Epoch training time 52.31076693534851\n",
      "Epoch: 5; Batch: 1; Loss: 0.020322288038095822;Training Acc:24.99%;Testing Acc:25.05%\n",
      "Epoch training time 52.27956247329712\n",
      "Epoch: 6; Batch: 1; Loss: 0.019753429069029663;Training Acc:27.11%;Testing Acc:26.82%\n",
      "Epoch training time 52.304787397384644\n",
      "Epoch: 7; Batch: 1; Loss: 0.01904657780560517;Training Acc:29.60%;Testing Acc:29.41%\n",
      "Epoch training time 52.33017420768738\n",
      "Epoch: 8; Batch: 1; Loss: 0.018216959152755046;Training Acc:33.24%;Testing Acc:33.15%\n",
      "Epoch training time 52.29850125312805\n",
      "Epoch: 9; Batch: 1; Loss: 0.01732082804309352;Training Acc:36.48%;Testing Acc:36.65%\n",
      "Epoch training time 52.30570411682129\n",
      "Epoch: 10; Batch: 1; Loss: 0.01641217301991274;Training Acc:39.28%;Testing Acc:39.87%\n",
      "Epoch training time 52.34854316711426\n",
      "Epoch: 11; Batch: 1; Loss: 0.015566805253218349;Training Acc:41.34%;Testing Acc:42.32%\n",
      "Epoch training time 52.38261413574219\n",
      "Epoch: 12; Batch: 1; Loss: 0.014943950111508399;Training Acc:43.77%;Testing Acc:44.84%\n",
      "Epoch training time 52.36032819747925\n",
      "Epoch: 13; Batch: 1; Loss: 0.014423940247872701;Training Acc:46.09%;Testing Acc:47.07%\n",
      "Epoch training time 52.28242254257202\n",
      "Epoch: 14; Batch: 1; Loss: 0.013961191146180411;Training Acc:47.80%;Testing Acc:48.73%\n",
      "Epoch training time 52.30264091491699\n",
      "Epoch: 15; Batch: 1; Loss: 0.01356593504057179;Training Acc:49.22%;Testing Acc:50.12%\n",
      "Epoch training time 52.24958515167236\n",
      "Epoch: 16; Batch: 1; Loss: 0.013237576573610495;Training Acc:50.47%;Testing Acc:51.23%\n",
      "Epoch training time 53.562206983566284\n",
      "Epoch: 17; Batch: 1; Loss: 0.012829061063449426;Training Acc:51.66%;Testing Acc:52.33%\n",
      "Epoch training time 52.455392360687256\n",
      "Epoch: 18; Batch: 1; Loss: 0.012502121242772456;Training Acc:52.82%;Testing Acc:53.52%\n",
      "Epoch training time 52.2252037525177\n",
      "Epoch: 19; Batch: 1; Loss: 0.012166134999232187;Training Acc:53.96%;Testing Acc:54.55%\n",
      "Epoch training time 52.26333141326904\n",
      "Epoch: 20; Batch: 1; Loss: 0.011855419674976707;Training Acc:55.01%;Testing Acc:55.60%\n",
      "Epoch training time 52.23559784889221\n",
      "Epoch: 21; Batch: 1; Loss: 0.011579749786170653;Training Acc:55.90%;Testing Acc:56.48%\n",
      "Epoch training time 52.28807020187378\n",
      "Epoch: 22; Batch: 1; Loss: 0.011311359008460227;Training Acc:56.77%;Testing Acc:57.30%\n",
      "Epoch training time 52.268537759780884\n",
      "Epoch: 23; Batch: 1; Loss: 0.011144100079514491;Training Acc:57.71%;Testing Acc:58.17%\n",
      "Epoch training time 52.19091463088989\n",
      "Epoch: 24; Batch: 1; Loss: 0.010807473352357612;Training Acc:58.49%;Testing Acc:58.89%\n",
      "Epoch training time 52.27335715293884\n",
      "Epoch: 25; Batch: 1; Loss: 0.010634675620081533;Training Acc:59.30%;Testing Acc:59.83%\n",
      "Epoch training time 52.26941633224487\n",
      "Epoch: 26; Batch: 1; Loss: 0.010420878979068696;Training Acc:60.11%;Testing Acc:60.53%\n",
      "Epoch training time 52.21137809753418\n",
      "Epoch: 27; Batch: 1; Loss: 0.01025637197376423;Training Acc:60.82%;Testing Acc:61.21%\n",
      "Epoch training time 52.17928218841553\n",
      "Epoch: 28; Batch: 1; Loss: 0.010091836059724038;Training Acc:61.44%;Testing Acc:61.90%\n",
      "Epoch training time 52.22876524925232\n",
      "Epoch: 29; Batch: 1; Loss: 0.009901608915622992;Training Acc:62.12%;Testing Acc:62.42%\n",
      "Epoch training time 52.247644901275635\n",
      "Epoch: 30; Batch: 1; Loss: 0.009828147255264775;Training Acc:62.73%;Testing Acc:62.93%\n",
      "Epoch training time 52.193050146102905\n",
      "Epoch: 31; Batch: 1; Loss: 0.009709516890911204;Training Acc:63.23%;Testing Acc:63.45%\n",
      "Epoch training time 52.238439321517944\n",
      "Epoch: 32; Batch: 1; Loss: 0.009550336632333794;Training Acc:63.81%;Testing Acc:63.90%\n",
      "Epoch training time 52.22554969787598\n",
      "Epoch: 33; Batch: 1; Loss: 0.009383454969972721;Training Acc:64.27%;Testing Acc:64.45%\n",
      "Epoch training time 52.216418504714966\n",
      "Epoch: 34; Batch: 1; Loss: 0.009190533920325922;Training Acc:64.65%;Testing Acc:64.75%\n",
      "Epoch training time 52.182289600372314\n",
      "Epoch: 35; Batch: 1; Loss: 0.009095728736960278;Training Acc:65.07%;Testing Acc:65.22%\n",
      "Epoch training time 52.2015118598938\n",
      "Epoch: 36; Batch: 1; Loss: 0.008980716760875291;Training Acc:65.50%;Testing Acc:65.44%\n",
      "Epoch training time 52.175459146499634\n",
      "Epoch: 37; Batch: 1; Loss: 0.008764001098601069;Training Acc:65.99%;Testing Acc:66.02%\n",
      "Epoch training time 52.453622341156006\n",
      "Epoch: 38; Batch: 1; Loss: 0.00873377945334397;Training Acc:66.44%;Testing Acc:66.48%\n",
      "Epoch training time 52.204679012298584\n",
      "Epoch: 39; Batch: 1; Loss: 0.008538197159541117;Training Acc:66.95%;Testing Acc:66.89%\n",
      "Epoch training time 52.2083044052124\n",
      "Epoch: 40; Batch: 1; Loss: 0.008512586544694481;Training Acc:67.42%;Testing Acc:67.40%\n",
      "Epoch training time 52.20153856277466\n",
      "Epoch: 41; Batch: 1; Loss: 0.00832122570135032;Training Acc:67.98%;Testing Acc:67.97%\n",
      "Epoch training time 53.17776036262512\n",
      "Epoch: 42; Batch: 1; Loss: 0.00816966917782187;Training Acc:68.38%;Testing Acc:68.26%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ### transform text into list of words\n",
    "\n",
    "    df = pd.read_pickle('glove.pkl')\n",
    "    _, words_lst, tags_lst = split_text(\"wsj1-18.training\")\n",
    "    tags = dict(zip(sorted(set(tags_lst)), np.arange(len(set(tags_lst)))))\n",
    "    tags['<PAD>'] = 45\n",
    "    vocab = dict(zip(sorted(set(words_lst)), np.arange(len(set(words_lst)))))\n",
    "    vocab['<PAD>'] = 912344\n",
    "    weighted_matrix = torch.load(\"weighed_matrix.pt\")\n",
    "    embedding_layer_const = create_emb_layer(weighted_matrix)\n",
    "    nb_layers = 2\n",
    "    nb_lstm_units = 1024\n",
    "    batch_size = 128    \n",
    "    lr = 1e-2\n",
    "    momentum = 0.9\n",
    "    dropout = 0.1\n",
    "    num_epoch = 100\n",
    "    model = LSTM(nb_layers=nb_layers,\n",
    "                 batch_size=batch_size,\n",
    "                 nb_lstm_units=nb_lstm_units,\n",
    "                 embedding_layer=embedding_layer_const,\n",
    "                 bidirectional=True,\n",
    "                 dropout=dropout)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    train_loader = DataLoader(TagDataset(train=True),\n",
    "                          batch_size=batch_size,\n",
    "                          # num_workers=4, # Mac M1 cannot use this\n",
    "                          collate_fn=collate_fn,\n",
    "                          shuffle=False,\n",
    "                          drop_last=True)\n",
    "    test_loader = DataLoader(TagDataset(train=False),\n",
    "                         batch_size=batch_size,\n",
    "                         # num_workers=4, # Mac M1 cannot use this\n",
    "                         collate_fn=collate_fn,\n",
    "                         shuffle=False,\n",
    "                         drop_last=True)\n",
    "    seq_len = 30\n",
    "    padding_idx = 912344\n",
    "\n",
    "\n",
    "    train(model=model, num_epoch=num_epoch)\n",
    "\n",
    "    # toy_training = [\n",
    "    #     (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    #     (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "    # ]\n",
    "    # toy_training_1 = [\n",
    "    #     \"The dog ate the apple\".split(),\n",
    "    #     \"Everybody read that book\".split()]\n",
    "    # x, y = next(iter(train_loader))\n",
    "    # # batch_in = torch.tensor([[[4],\n",
    "    # #                           [5],\n",
    "    # #                           [912344],\n",
    "    # #                           [912344]],\n",
    "    # #\n",
    "    # #                          [[6],\n",
    "    # #                           [912344],\n",
    "    # #                           [912344],\n",
    "    # #                           [912344]],\n",
    "    # #\n",
    "    # #                          [[7],\n",
    "    # #                           [8],\n",
    "    # #                           [9],\n",
    "    # #                           [10]]])\n",
    "    # # Y = [[\"CC\", \"CD\", \"DT\"], [\"EX\"], [\"JJ\", \"IN\", \"JJ\", \"JJR\"]]\n",
    "    # model = LSTM(nb_layers=nb_layers,\n",
    "    #              batch_size=batch_size,\n",
    "    #              nb_lstm_units=nb_lstm_units,\n",
    "    #              embedding_layer=embedding_layer_const,\n",
    "    #              bidirectional=False)\n",
    "    # out = model(x)\n",
    "    # print(f\"out dim {out.size()} \\n Out: {out}\")\n",
    "    # # out dim torch.Size([32, 52, 45])\n",
    "    # # y.shape : [32, 52]\n",
    "    # loss = model.loss(out, y)\n",
    "    # print(f\"loss: \".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "embedding_layer.weight \t torch.Size([912345, 50])\n",
      "lstm.weight_ih_l0 \t torch.Size([4096, 50])\n",
      "lstm.weight_hh_l0 \t torch.Size([4096, 1024])\n",
      "lstm.bias_ih_l0 \t torch.Size([4096])\n",
      "lstm.bias_hh_l0 \t torch.Size([4096])\n",
      "lstm.weight_ih_l0_reverse \t torch.Size([4096, 50])\n",
      "lstm.weight_hh_l0_reverse \t torch.Size([4096, 1024])\n",
      "lstm.bias_ih_l0_reverse \t torch.Size([4096])\n",
      "lstm.bias_hh_l0_reverse \t torch.Size([4096])\n",
      "lstm.weight_ih_l1 \t torch.Size([4096, 2048])\n",
      "lstm.weight_hh_l1 \t torch.Size([4096, 1024])\n",
      "lstm.bias_ih_l1 \t torch.Size([4096])\n",
      "lstm.bias_hh_l1 \t torch.Size([4096])\n",
      "lstm.weight_ih_l1_reverse \t torch.Size([4096, 2048])\n",
      "lstm.weight_hh_l1_reverse \t torch.Size([4096, 1024])\n",
      "lstm.bias_ih_l1_reverse \t torch.Size([4096])\n",
      "lstm.bias_hh_l1_reverse \t torch.Size([4096])\n",
      "hidden_to_tag.weight \t torch.Size([45, 2048])\n",
      "hidden_to_tag.bias \t torch.Size([45])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "PATH = \"bidirectional_model.pt\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
