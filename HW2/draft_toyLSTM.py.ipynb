{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import unittest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "def load_embedding(filename='glove.6B.50d.txt'):\n",
    "    \"\"\"\n",
    "    Load embedding for the training and\n",
    "    :return: dataframe, words\n",
    "    \"\"\"\n",
    "    # creat column names\n",
    "    num = np.arange(51)\n",
    "    num_str = list(map(str, num))\n",
    "    list_name = list(map(lambda x: \"dim_\" + x, num_str))\n",
    "    df = pd.read_csv(\"glove.6B.50d.txt\", sep=\" \", quoting=csv.QUOTE_NONE,\n",
    "                     header=None, encoding='utf-8',\n",
    "                     names=list_name)\n",
    "    df.rename({'dim_0': 'token'}, axis=1, inplace=True)\n",
    "    words = df.token.to_list()\n",
    "    # add padding embedding\n",
    "    df.loc['<PAD>'] = np.zeros(50)\n",
    "    df.set_index('token', inplace=True)\n",
    "    df.to_pckle(\"glove.pkl\")\n",
    "    return df, words\n",
    "def word_to_embedding(target_vocab, pre_train):\n",
    "    \"\"\"\n",
    "\n",
    "    :param pre_train: pd.DataFrame pre-trained dataframe\n",
    "    :param target_vocab: list/ array of tokens need to be transformed\n",
    "    :return: transformed matrix, result dictionary for the unique tokens\n",
    "    \"\"\"\n",
    "    matrix_len = len(target_vocab)\n",
    "    weighted_matrix = np.zeros((matrix_len + 1, 50))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(target_vocab):\n",
    "        try:\n",
    "            weighted_matrix[i] = pre_train.loc[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weighted_matrix[i] = np.random.normal(size=50)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Finished {}th words\".format(i))\n",
    "    return weighted_matrix\n",
    "def create_emb_layer(weighted_matrix1, non_trainable=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param weighted_matrix1: tensor matrix\n",
    "    :param non_trainable:\n",
    "    :return: emb_layer type embedding\n",
    "    \"\"\"\n",
    "    input_shape, embedding_dim = weighted_matrix1.shape\n",
    "    emb_layer = nn.Embedding.from_pretrained(weighted_matrix1,\n",
    "                                             padding_idx=input_shape - 1)\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    return emb_layer\n",
    "def split_text(text_file):\n",
    "    \"\"\"\n",
    "\n",
    "    :param text_file: training file\n",
    "    :return: DIC, TOKENS and TAGS\n",
    "\n",
    "    \"\"\"\n",
    "    with open(text_file, mode=\"r\") as file:\n",
    "        text_f = file.read()\n",
    "        text_f_lst = text_f.split()\n",
    "        file.close()\n",
    "    keys, values = text_f_lst[::2], text_f_lst[1::2]\n",
    "    result_dic = dict(zip(keys, values))\n",
    "    return result_dic, keys, values\n",
    "def prepare_seq(seq_list, dictionary):\n",
    "    \"\"\"\n",
    "    embedding and padded a sequence, given its relating dictionary\n",
    "    :return: padded sequence in numerical numbers\n",
    "    \"\"\"\n",
    "    embedded = []\n",
    "    for batch in seq_list:\n",
    "        empty_lst = [dictionary[tag] for tag in batch]\n",
    "        embedded.append(empty_lst)\n",
    "    # convert to a list of tensor\n",
    "    embedded = [torch.tensor(seq) for seq in embedded]\n",
    "    padded = nn.utils.rnn.pad_sequence(embedded,\n",
    "                                       batch_first=True,\n",
    "                                       padding_value=dictionary['<PAD>'])\n",
    "    return padded"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nb_layers, batch_size, nb_lstm_units, embedding_layer,\n",
    "                 bidirectional=False,\n",
    "                 dropout=0,\n",
    "                 embedding_dim=50):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layer = None\n",
    "        self.result_dic, self.words_lst, self.tags_lst = split_text(\"wsj1-18.training\")\n",
    "        self.vocab = dict(zip(sorted(set(self.words_lst)),\n",
    "                              np.arange(len(set(self.words_lst)))))\n",
    "        self.tags = dict(zip(sorted(set(self.tags_lst)),\n",
    "                             np.arange(len(set(self.tags_lst)))))\n",
    "        self.vocab['<PAD>'] = len(set(self.words_lst))\n",
    "        self.tags['<PAD>'] = len(set(self.tags_lst))\n",
    "        self.padding_idx = self.vocab['<PAD>']\n",
    "        self.nb_layers = nb_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout if nb_layers > 1 else 0\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "        # don't count the pad for the tags\n",
    "        self.nb_tags = len(self.tags) - 1\n",
    "\n",
    "        # build actual NN\n",
    "        self.__build_model()\n",
    "\n",
    "    def __build_model(self):\n",
    "        # design LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            batch_first=True,\n",
    "            num_layers=self.nb_layers,\n",
    "            bidirectional=self.bidirectional,\n",
    "            dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # output layer which project back to tag space\n",
    "        self.hidden_to_tag = nn.Linear(self.nb_lstm_units * 2\n",
    "                                       if self.bidirectional\n",
    "                                       else self.nb_lstm_units\n",
    "                                       , self.nb_tags)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # init hidden layers and input sequence length\n",
    "        h0 = torch.rand(self.nb_layers, input.size(0), self.nb_lstm_units)\n",
    "        c0 = torch.rand(self.nb_layers, input.size(0), self.nb_lstm_units)\n",
    "        input_lengths = torch.all(input != self.padding_idx, dim=2)\\\n",
    "            .sum(dim=1).flatten()\n",
    "\n",
    "        # -------------------\n",
    "        # 1. embed the input\n",
    "        # Dim transformation: (batch_size, seq_len, 1) -> (batch_size, seq_len,\n",
    "        # embedding_dim)\n",
    "        input = self.dropout_layer(self.embedding_layer(input))\n",
    "        input = input.squeeze(2)\n",
    "        # -------------------\n",
    "        # 2.  Run through LSTM\n",
    "        # Dim transformation: (B,L, embedding_dim) -> (B, L, LSTM_units)\n",
    "        input = torch.nn.utils.rnn.pack_padded_sequence(input,\n",
    "                                                        input_lengths,\n",
    "                                                        batch_first=True,\n",
    "                                                        enforce_sorted=False)\n",
    "        # now run through LSTM\n",
    "        input = input.float()\n",
    "        out, (h0, c0) = self.lstm(input, (h0, c0))  # undo the packing operation\n",
    "        out, len_unpacked = nn.utils.rnn.pad_packed_sequence(out,\n",
    "                                                             batch_first=True)\n",
    "        # -------------------\n",
    "        # 3.  Apply FC linear layer\n",
    "        # linear layer\n",
    "        out = out.view(-1,\n",
    "                       out.size(-1))  # (batch_size, seq_len, nb_lstm_units) -> (batch_size * seq_len, nb_lstm_units)\n",
    "        out = self.hidden_to_tag(out)  # (batch_size * seq_len, nb_lstm_units) -> (batch_size * seq_len, nb_tags)\n",
    "\n",
    "        # reshape into (batch_size,  seq_len, nb_lstm_units)\n",
    "        out = out.view(self.batch_size, -1, self.nb_tags)\n",
    "        # -------------------\n",
    "        # 4.  softmax to transfer it to probability\n",
    "        Y_hat = F.log_softmax(out.float(), dim=2)\n",
    "        return Y_hat\n",
    "\n",
    "    def loss(self, Y_hat, Y):\n",
    "        # NLL(tensor log_softmax output, target index list)\n",
    "        # flatten out all labels\n",
    "        Y = prepare_seq(Y, self.tags)\n",
    "        Y = Y.flatten()\n",
    "        # flatten all predictions\n",
    "        Y_hat = Y_hat.view(-1, len(self.tags) - 1)\n",
    "        # create a mask that filter '<PAD>;\n",
    "        tag_token = self.tags['<PAD>']\n",
    "        mask = (Y < tag_token)\n",
    "        mask_idx = torch.nonzero(mask.float())\n",
    "        Y_hat = Y_hat[mask_idx].squeeze(1)\n",
    "        Y = Y[mask_idx].squeeze(1)\n",
    "        loss = nn.NLLLoss()\n",
    "        result = loss(Y_hat, Y)\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "### transform text into list of words\n",
    "df = pd.read_pickle('glove.pkl')\n",
    "_, words_lst, tags_lst = split_text(\"wsj1-18.training\")\n",
    "tags = dict(zip(sorted(set(tags_lst)), np.arange(len(set(tags_lst)))))\n",
    "tags['<PAD>'] = len(tags)\n",
    "weighted_matrix = torch.load(\"weighed_matrix.pt\")\n",
    "embedding_layer_const = create_emb_layer(weighted_matrix)\n",
    "nb_layers = 2\n",
    "nb_lstm_units = 32\n",
    "batch_size = 3\n",
    "seq_len = 4\n",
    "padding_idx = 912344\n",
    "toy_training = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "batch_in = torch.tensor([[[4],\n",
    "                          [5],\n",
    "                          [912344],\n",
    "                          [912344]],\n",
    "\n",
    "                         [[6],\n",
    "                          [912344],\n",
    "                          [912344],\n",
    "                          [912344]],\n",
    "\n",
    "                         [[7],\n",
    "                          [8],\n",
    "                          [9],\n",
    "                          [10]]])\n",
    "Y = [[\"CC\", \"CD\", \"DT\"], [\"EX\"], [\"JJ\", \"IN\", \"JJ\", \"JJR\"]]\n",
    "model = LSTM(nb_layers=nb_layers,\n",
    "             batch_size=batch_size,\n",
    "             nb_lstm_units=nb_lstm_units,\n",
    "             embedding_layer=embedding_layer_const,\n",
    "             bidirectional=False)\n",
    "out = model(batch_in)\n",
    "pred = out.max(dim=2, keepdim=False)[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 8,  9, 10, 45, 11, 45, 45, 45, 14, 13, 14, 15])"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = prepare_seq(Y, tags)\n",
    "Y = Y.flatten()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([21, 21, 21, 26, 18, 18, 26, 26, 21, 21, 21, 26])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.flatten()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.6738,  0.7241, -0.3422],\n        [ 0.0000,  0.0000,  0.0000],\n        [-2.1168,  3.2442,  0.0000],\n        [ 1.6979, -0.0000, -0.0000],\n        [-1.5603, -0.6838,  1.5060],\n        [-2.6656,  0.9039,  0.0000],\n        [ 0.0732, -0.8188, -1.5164],\n        [-1.0088, -0.1575,  0.5954],\n        [-0.0000,  1.1957,  2.6407],\n        [-2.1896, -0.7538,  0.9604]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}